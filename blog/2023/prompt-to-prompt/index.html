<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Prompt to Prompt | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="Text-based image editing via cross-attention mechanisms - the research of hyperparameters and novel mechanisms to enhance existing frameworks"/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/blog/2023/prompt-to-prompt/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/staging/assets/js/distillpub/template.v2.js"></script> <script src="/staging/assets/js/distillpub/transforms.v2.js"></script> <script src="/staging/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Prompt to Prompt",
      "description": "Text-based image editing via cross-attention mechanisms - the research of hyperparameters and novel mechanisms to enhance existing frameworks",
      "published": "November 7, 2023",
      "authors": [
        {
          "author": "Carla Lorente",
          "authorURL": "https://www.linkedin.com/in/carla-lorente/",
          "affiliations": [
            {
              "name": "MIT EECS 2025",
              "url": ""
            }
          ]
        },
        {
          "author": "Linn Bieske",
          "authorURL": "https://www.linkedin.com/in/linn-bieske-189b9b138//",
          "affiliations": [
            {
              "name": "MIT EECS 2025",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/staging/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Prompt to Prompt</h1> <p>Text-based image editing via cross-attention mechanisms - the research of hyperparameters and novel mechanisms to enhance existing frameworks</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#research-questions">Research questions</a></div> <div><a href="#methodology">Methodology</a></div> <div><a href="#conclusion">Conclusion</a></div> <div><a href="#references">References</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Recently, the techniques to edit images have advanced from methodologies that require the user to edit individual pixels to deep learning-based image editing. The latter employ for example large image generation models (e.g., stable diffusion models). While these deep learning-based image editing techniques initially required the user to mark particular areas which should be edited (Nichol et al., 2021 <d-cite key="nichol2021glide"></d-cite>; Avrahami et al., 2022a<d-cite key="avrahami2022blendeddiffusion"></d-cite>; Ramesh et al., 2022), recently the work by (Hertz et al, 2022 <d-cite key="hertz2022prompttoprompt"></d-cite>) has shown that this becomes unnecessary. Instead, image editing can be performed using a cross-attention mechanism. In particular, the proposed prompt-to-prompt editing framework enables the controlling of image edits by text only. The section below provides an overview of how this prompt-to-prompt framework works (Figure 1, by (Hertz et al, 2022<d-cite key="hertz2022prompttoprompt"></d-cite>)).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/1-cross_attention_masks-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/1-cross_attention_masks-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/1-cross_attention_masks-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/1-cross_attention_masks.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><em>Figure 1: Cross-attention method overview. Top: visual and textual embedding are fused using cross-attention layers that produce attention maps for each textual token. Bottom: we control the spatial layout and geometry of the generated image using the attention maps of a source image. This enables various editing tasks through editing the textual prompt only. When swapping a word in the prompt, we inject the source image maps Mt, overriding the target maps M ∗ t . In the case of adding a refinement phrase, we inject only the maps that correspond to the unchanged part of the prompt. To amplify or attenuate the semantic effect of a word, we re-weight the corresponding attention map. (Hertz et al, 2022 <d-cite key="hertz2022prompttoprompt"></d-cite>).</em></p> <p>While this proposed framework has significantly advanced the image editing research field, its performance leaves still room for improvement such that open research questions remain. For example, when performing an image editing operation that changes the hair color of a woman, significant variability across the woman’s face can be observed (Figure 2). This is undesirable, as the user would expect to see the same female face across all four images.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/2-Experimentation_proposed_prompt_to_prompt-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/2-Experimentation_proposed_prompt_to_prompt-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/2-Experimentation_proposed_prompt_to_prompt-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/2-Experimentation_proposed_prompt_to_prompt.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><em>Figure 2: Experimentation with the proposed prompt-to-prompt image editing framework presented by (Hertz et al, 2022<d-cite key="hertz2022prompttoprompt"></d-cite>). The faces of the women show significant variability even though they should remain invariant across all four generated/ edited images.</em></p> <p>Within our work, we will start to further benchmark the performance of the proposed framework, explore the impact of its hyperparameters on the image editing process, and research opportunities to improve the underlying cross-attention mechanism.</p> <h2 id="research-questions">Research questions</h2> <p>Our research question is threefold and contains both realistic and ambitious aspects.</p> <ul> <li><strong>Benchmark:</strong> First, we intend to further benchmark the capabilities of the proposed framework (e.g., across defined dimensions such as applicability to different domains, robustness of editing, realism, and alignment to user prompt and intention).</li> <li><strong>Hyperparameter investigation:</strong> Second, the currently proposed prompt-to-prompt framework does not explore and quantify the impact of its different hyperparameters on its editing performance (time steps of diffusion for each cross-attention mask, scaling factor, …)</li> <li><strong>Enhanced attention mechanism:</strong> Initial evaluation of the prompt-to-prompt framework made us observe shortcomings including the distortion of the image across editing steps. Therefore, we will explore approaches to strengthen the underlying cross-attention mechanism (e.g., by exploring regularization techniques). The exact mechanism which could lead to an enhanced image editing performance is subject to research.</li> </ul> <h2 id="methodology">Methodology</h2> <p>To perform our research, we plan to build upon the code which complemented the paper published by (Hertz et al, 2022 <d-cite key="hertz2022prompttoprompt"></d-cite>, <a href="https://github.com/google/prompt-to-prompt/">Link to code</a>). Concretely, we will rely on a stable diffusion model from hugging face which we will access via Python. No model training is required as we will solely work with attention layers that capture spatial information about the images. By now, we have reviewed and tested the code implementation, resolved any encountered bugs, and have started the exploration of the functionalities of the published repository. This makes us feel comfortable that our ambitions are feasible.</p> <p>To achieve all three of our realistic and ambitious research goals we plan to undertake the following steps:</p> <ul> <li><strong>Benchmarking:</strong> First, we will define 5 categories of interests (e.g., human faces, interior designs, animals, food, and transportation) for which we will test both, the image generation process of the stable diffusion model itself as well as the image editing performance of the cross-attention mechanisms presented by (Hertz et al, 2022 <d-cite key="hertz2022prompttoprompt"></d-cite>). The judge of the benchmarking process will be ourselves (Carla and Linn), since this will help us further understand the shortcomings of the existing framework.</li> <li><strong>Hyperparameter investigation:</strong> For a selection of the defined categories of interest we will perform a hyperparameter study. This will entail two scenarios: 1. studying the impact of each individual hyperparameter independently to research its individual impact on the quality of the edited images. 2. Studying the interdependence of the hyperparameters by performing a grid search. The outcome of step (1) would inform reasonable search spaces for each hyperparameter.</li> <li><strong>Enhanced attention mechanism:</strong> We have the ambition to explore opportunities to improve the performance of the cross-attention image editing mechanism beyond the tuning of hyperparameters. Therefore, we will research approaches to improve the framework. Each architecture change of the cross-attention algorithm will be benchmarked to assess whether a performance improvement is possible. Here, we may look into expanding the user input to a larger group of people beyond our team</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>This research endeavors to push the boundaries of text-based image editing, with the potential to significantly streamline creative workflows and introduce a new level of user accessibility to image manipulation. By delving into the intricacies of the prompt-to-prompt framework and its underlying hyperparameters, the research not only paves the way for more robust and realistic image manipulations but also opens up new avenues for creative expression and accessibility in digital media.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/staging/assets/bibliography/2023-11-07-prompt-to-prompt.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>