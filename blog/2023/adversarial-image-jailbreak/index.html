<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Project Proposal | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="Using Adversarial Images to Jailbreak Large Visual Language Models"/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/blog/2023/adversarial-image-jailbreak/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/staging/assets/js/distillpub/template.v2.js"></script> <script src="/staging/assets/js/distillpub/transforms.v2.js"></script> <script src="/staging/assets/js/distillpub/overrides.js"></script> </head> <d-front-matter> <script async type="text/json">{
      "title": "Project Proposal",
      "description": "Using Adversarial Images to Jailbreak Large Visual Language Models",
      "published": "November 9, 2023",
      "authors": [
        {
          "author": "Julie Steele",
          "authorURL": "mailto:jssteele@mit.edu",
          "affiliations": [
            {
              "name": "MIT",
              "url": ""
            }
          ]
        },
        {
          "author": "Spencer Yandrofski",
          "authorURL": "mailto:spencery@mit.edu",
          "affiliations": [
            {
              "name": "MIT",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/staging/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Project Proposal</h1> <p>Using Adversarial Images to Jailbreak Large Visual Language Models</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#proposal">Proposal</a></div> <div><a href="#citations">Citations</a></div> <div><a href="#footnotes">Footnotes</a></div> <div><a href="#code-blocks">Code Blocks</a></div> <div><a href="#layouts">Layouts</a></div> <div><a href="#other-typography">Other Typography?</a></div> </nav> </d-contents> <h2 id="proposal--using-adversarial-images-to-jailbreak-large-visual-language-models">Proposal : Using Adversarial Images to Jailbreak Large Visual Language Models</h2> <p>We hope to study using adversarially crafted images as inputs to large visual language models (like gpt4, where one can input an image) to jailbreak the language model. Jailbreaking entails bypassing alignment efforts for the model not to speak on dangerous/mean topics. Creating adversarial images to trick image classifiers has been widely studied, and methods including fast gradient sign method, Carlini-Wagner’s L2 attack, Biggio’s attack, Szegedy′s attack, and more (see https://arxiv.org/pdf/1711.00117.pdf, https://link.springer.com/article/10.1007/s11633-019-1211-x) have been effective. There have also been successful efforts in optimizing token inputs to jailbreak language models. The recent creation of visual language models allows for an oportunity to combine adversarial images and jailbreaking.</p> <p>We will investigate the applicability of each of these attacks for visual language models, and then compare a couple of them on effectiveness at jailbreaking the model. Will some work unexpectedly better/worse compared to image classification adversarial attacks? Why? We would start with trying white-box attacks (viewing the weights of the visual language model). One question we will have to tackle is what is a good measure of jailbreaking success we have (as opposed to classification accuricary), and if we can find an objective measure to use in the model. We would use pretrained open source MiniGPT4 for the experiments.</p> <p>All parts of this project are very subject to change, and we would love ideas and mentorship from course staff!</p> <h2 id="other-ideas">Other Ideas</h2> <p>Training a GAN: model 1 makes adversarial images, model 2 finds the fake Jailbreaking an LLM, experimenting over different levels to do the optimization (tokens? post-embedding?) Adversarial images for jailbreaking language models (see https://arxiv.org/abs/2306.13213): This paper compares text attacks for jailbreaking and image attacks. Since images are differentiable, they work better. Adversarial training and robustness certification are two methods to try to fix this, but likely not to prevent image attacks.</p> <h2 id="related-work">Related Work</h2> <ul> <li>https://arxiv.org/abs/2306.13213 **extremely related, building off of</li> <li>https://arxiv.org/pdf/1711.00117.pdf</li> <li>https://arxiv.org/pdf/2002.02196.pdf</li> <li>https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/reports/6906148.pdf</li> <li>https://mpost.io/openai-develops-jailbreak-gan-to-neutralize-prompt-hackers-rumors-says/</li> <li>https://arxiv.org/abs/2307.15043</li> <li>https://ieeexplore.ieee.org/abstract/document/7727230?casa_token=82pyRsetYb0AAAAA:GsItW94vrH-aqxxl8W365qG_CBDt_lSyMfCn33bD32HNonSt2LKd_0QZLve7rnrg9fmeLmqYsw</li> <li>https://link.springer.com/article/10.1007/s11633-019-1211-x</li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/staging/assets/bibliography/2023-11-9-adversarial-image-jailbreak.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>