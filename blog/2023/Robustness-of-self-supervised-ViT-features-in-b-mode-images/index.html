<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Robustness of self supervised ViT features in b-mode images | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="Project proposal for 6.S898 Deep Learning MIT class"/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/blog/2023/Robustness-of-self-supervised-ViT-features-in-b-mode-images/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/staging/assets/js/distillpub/template.v2.js"></script> <script src="/staging/assets/js/distillpub/transforms.v2.js"></script> <script src="/staging/assets/js/distillpub/overrides.js"></script> </head> <d-front-matter> <script async type="text/json">{
      "title": "Robustness of self supervised ViT features in b-mode images",
      "description": "Project proposal for 6.S898 Deep Learning MIT class",
      "published": "November 9, 2023",
      "authors": [
        {
          "author": "Roger Pallares Lopez",
          "authorURL": "https://www.linkedin.com/in/rogerpallareslopez/",
          "affiliations": [
            {
              "name": "Mechanical Engineering Department, MIT",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/staging/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Robustness of self supervised ViT features in b-mode images</h1> <p>Project proposal for 6.S898 Deep Learning MIT class</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#project-description">Project Description</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>B-mode ultrasound imaging is a widely employed medical imaging technique that uses high-frequency sound waves to produce visual representations of the internal structures of the human body. Its main advantages are its ability to produce real-time images, its portability, low cost, and especially the fact that is noninvasive and safe (non-radiating). However, it is an imaging modality that carries a very high noise-to-signal ratio. Speckle noise, out-of-plane movement, and high variability in image reconstruction across devices make the resulting images complex to interpret and diagnose <d-cite key="us"></d-cite>. As an example, the following image shows a b-mode ultrasound image.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/img1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/img1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/img1-1400.webp"/> <img src="/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/img1.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Ultrasound b-mode image of the upper arm with the main physiology annotated. </div> <p>Self-supervised Vision Transformers (ViT) have emerged as a powerful tool to extract deep features for a variety of downstream tasks, such as classification, segmentation, or image correspondence. Especially, DINO architectures <d-cite key="dino1"></d-cite> <d-cite key="dino2"></d-cite> have exhibited striking properties, where its features present localized semantic information shared across related object categories, even in zero-shot methodologies <d-cite key="dino_feat"></d-cite>. Consequently, the aforementioned properties of DINO may allow us to develop efficient yet simple methods for b-mode ultrasound image interpretation, without the need for an expert or ground truth labels.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/img2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/img2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/img2-1400.webp"/> <img src="/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/img2.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> DINOv2 segmentation of different objects. Note the consistency between parts of real vs toy/drawn objects of the same category. Adapted from <d-cite key="dino2"></d-cite>. </div> <h2 id="project-description">Project Description</h2> <p>We propose analyzing the performance and robustness of DINO in b-mode ultrasound images of the upper and lower limbs. We note that this dataset features a set of images with a high noise-to-signal ratio, which is a property that DINO has not yet been tested against. In particular, we will focus on assessing DINO in segmentation and correspondence tasks in a zero-shot approach. We will perform so by applying dimensionality reduction algorithms and subsequent clustering to the deep features of the model.</p> <p>For the segmentation task, we will try to segment bone and fascia tissues from arm images obtained from a subject while is moving. For the correspondence task, we will try to find correspondence between bones and fascia of images from 4 different sources: arm (subject 1 device 1), arm (subject 2 device 1), arm (subject 1 device 2), and leg (subject 1 device 2).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/img3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/img3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/img3-1400.webp"/> <img src="/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/img3.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Example of one image of each source. A) Labeled bone and fascia. B) Arm (subject 1 device 1). C) Arm (subject 2 device 1). D) Arm (subject 1 device 2). E) Leg (subject 1 device 2) </div> <p>In addition, we aim to explore how these features change from a shallower to a deeper layer, trying to understand what positional and semantic information they carry. Finally, to further test and challenge DINO in an even more unfavorable scenario, we will gradually include adversarial noise in our dataset, assessing how the performance changes.</p> <p>In order to assess the efficacy of the model in all the aforementioned tasks and tests, both qualitative and quantitative methods will be employed. Qualitatively, we will plot clusters and segmented images. Quantitatively, we will label bone and fascia in images from the presented 4 sources and compute accuracy, Dice, and IoU metrics. Through all these experiments, we hope to gain insights into the feasibility of implementing DINO models in real-world medical imaging applications.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/staging/assets/bibliography/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>