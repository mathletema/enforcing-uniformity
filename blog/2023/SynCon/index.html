<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Contrastive Learning with Dynamically Weighted Synthetic Images | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="Final Project Proposal"/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/blog/2023/SynCon/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/staging/assets/js/distillpub/template.v2.js"></script> <script src="/staging/assets/js/distillpub/transforms.v2.js"></script> <script src="/staging/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Contrastive Learning with Dynamically Weighted Synthetic Images",
      "description": "Final Project Proposal",
      "published": "November 9, 2023",
      "authors": [
        {
          "author": "Fiona Cai",
          "authorURL": "",
          "affiliations": [
            {
              "name": "MIT",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/staging/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Contrastive Learning with Dynamically Weighted Synthetic Images</h1> <p>Final Project Proposal</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#final-project-proposal">Final Project Proposal</a></div> </nav> </d-contents> <h2 id="final-project-proposal">Final Project Proposal</h2> <p>Powerful text-to-image generation models enable the synthesis of high-quality images from textual descriptions. Recent advancements have led to research that leverages generative AI models to create synthetic data to provide additional support for various learning tasks</p> <d-cite key="voetman2023big, ruiz2023dreambooth, zhang2023prompt, ramesh2021zeroshot"></d-cite> <p>. When properly configured, these models can create synthetic images that enable self-supervised methods to match or even exceed performance for challenging discriminative tasks relative to training on real images <d-cite key="tian2023stablerep, azizi2023synthetic"></d-cite>. Synthetic images are also highly beneficial in few-shot learning settings since they can be used as support images to augment the training data set distribution and improve model performance on downstream tasks <d-cite key="sariyildiz2023fake, he2023synthetic"></d-cite>. %They can also address the common challenges of data scar city and diversity in certain real-world domains <d-cite key="udandarao2023susx"></d-cite>.</p> <p>Despite these advancements, challenges persist, particularly regarding the variation in quality of generated synthetic images across diverse domains and datasets. Generative AI models often fail to capture fine-grain details, especially in complex scenarios that require additional context <d-cite key="marcus2022preliminary"></d-cite>. Synthetic support images do not always completely capture the domain characteristics of the true data distribution, and while they may improve performance in certain classification scenarios, they can result in less accurate models in other domains <d-cite key="udandarao2023susx, jahanian2022generative"></d-cite>. This drawback makes it challenging to effectively integrate synthetic support set samples in the learning process, especially when used for difficult vision tasks. Existing methods often treat synthetic images as if they were as informative as real images in the training process <d-cite key="ren2017crossdomain, azizi2023synthetic"></d-cite>. We hypothesize that it would be better to adjust the utilization of synthetic images based on their quality and relevance to the corresponding real data. We explore this issue by developing a method that combines supervised contrastive learning (SupCon) <d-cite key="khosla2021supervised"></d-cite> with weighted representation learning. The weighting parameter modulates the contribution of synthetic images to the learned embedding.</p> <p>For my final project, I am proposing Contrastive Learning with Dynamically Weighted Synthetic Images (SynCon, name pending?) through a novel contrastive loss function designed to learn uncertainty weights for synthetic images. These weights would be dynamically learned and would reflect the quality and contribution of synthetic images to class representation. Extending the supervised contrastive method, we consider many positive examples per anchor, where positive examples now include both real and synthetic images per class. For each class, the optimization pulls together normalized embeddings of real images of the same class in the usual way. It also pulls together real and synthetic images from the same class but scaled by a weighting value. I will experiment with two main methods of learning the weighting hyperparameter per synthetic image class, specifically regular SGD and using a hypernetwork <d-cite key="ha2016hypernetworks"></d-cite>. I will test the loss function by using DALL-E to generate synthetic images per class and will focus on popular vision datasets such as Flowers102, FGVCAircraft, Birdsnap for testing and evaluation. I will use downstream classification accuracy as a metric to evaluate the proposed loss function and compare to baselines SupCon with and without synthetic images. This is the proposed loss function equation:</p> \[\begin{equation} \label{eq:3} \mathcal{L}_{SynCon} = \sum_{i\in S(i)}\Bigl(\frac{-1}{|P(i)|}\sum_{p\in P(i)} log\frac{exp(\frac{v_i^Tv_p}{\tau})}{\sum_{a\in A(i)}exp(\frac{v_i^Tv_a}{\tau})} + \frac{-w}{|U(i)|}\sum_{u\in U(i)}log \frac{exp(\frac{v_i^Tv_u}{\tau})}{\sum_{a\in A(i)}exp(\frac{v_i^Tv_a}{\tau})}\Bigl) \end{equation}\] <p>Here, we consider the set $S$ as all the real images in the multi-view batch such that $S \subseteq X$. Then for each anchor image $i \in S(i)$, $P(i)$ refers to the set of all indices of positive pairs from the same class that are real images so $P(i) \subseteq X$. The left term of the outer summation is exactly identical to the SupCon loss function but applied only to the positive examples that are real images.</p> <p>The right term introduces $w \in [0, 1]$, a weighting hyperparameter that provides a mechanism to modulate the influence of the embeddings of support set data in the learning process. The current loss setup uses the same weighting per class. $U(i)$ refers to the set of all indices of inputs that are in the same class as anchor $i$ but are support set images. Finally, the set $A(i)$ remains as described above. Effectively, each normal input anchor $i$ contributes to the UniCon loss through the sum of the SupCon loss and a weighted sum of all the similarities between the anchor and its corresponding support set images.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/staging/assets/bibliography/2023-11-09-SynCon.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>