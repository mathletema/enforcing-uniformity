<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Iterated Representation Learning | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="Representation learning is a subfield of deep learning focused on learning meaningful lower-dimensional embeddings of input data, and rapidly emerging to popularity for its efficacy with generative models. However, most representation learning techniques, such as autoencoders and variational autoencoders, learn only one embedding from the input data, which is then used to either reconstruct the original data or generate new samples. This project seeks to study the utility of a proposed iterated representation learning framework, which repeatedly trains new latent space embeddings based on the data outputted from the last round of representation. In particular, we seek to examine whether the performance of this iterated approach on a model and input dataset are indicative of any robustness qualities of the model and latent embedding space, and potentially derive a new framework for evaluating representation stability."/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/blog/2023/Iterated-Representation-Learning/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/staging/assets/js/distillpub/template.v2.js"></script> <script src="/staging/assets/js/distillpub/transforms.v2.js"></script> <script src="/staging/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Iterated Representation Learning",
      "description": "Representation learning is a subfield of deep learning focused on learning meaningful lower-dimensional embeddings of input data, and rapidly emerging to popularity for its efficacy with generative models. However, most representation learning techniques, such as autoencoders and variational autoencoders, learn only one embedding from the input data, which is then used to either reconstruct the original data or generate new samples. This project seeks to study the utility of a proposed iterated representation learning framework, which repeatedly trains new latent space embeddings based on the data outputted from the last round of representation. In particular, we seek to examine whether the performance of this iterated approach on a model and input dataset are indicative of any robustness qualities of the model and latent embedding space, and potentially derive a new framework for evaluating representation stability.",
      "published": "November 9, 2023",
      "authors": [
        {
          "author": "Angela Li",
          "authorURL": "https://www.linkedin.com/in/angelayli/",
          "affiliations": [
            {
              "name": "Harvard University",
              "url": ""
            }
          ]
        },
        {
          "author": "Evan Jiang",
          "authorURL": "https://www.linkedin.com/in/evanjiang1/",
          "affiliations": [
            {
              "name": "Harvard University",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/staging/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Iterated Representation Learning</h1> <p>Representation learning is a subfield of deep learning focused on learning meaningful lower-dimensional embeddings of input data, and rapidly emerging to popularity for its efficacy with generative models. However, most representation learning techniques, such as autoencoders and variational autoencoders, learn only one embedding from the input data, which is then used to either reconstruct the original data or generate new samples. This project seeks to study the utility of a proposed iterated representation learning framework, which repeatedly trains new latent space embeddings based on the data outputted from the last round of representation. In particular, we seek to examine whether the performance of this iterated approach on a model and input dataset are indicative of any robustness qualities of the model and latent embedding space, and potentially derive a new framework for evaluating representation stability.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#background">Background</a></div> <div><a href="#irl-framework">IRL Framework</a></div> <ul> <li><a href="#irl-for-aes">IRL for AEs</a></li> <li><a href="#irl-for-vaes">IRL for VAEs</a></li> </ul><div><a href="#potential-questions-and-hypotheses">Potential Questions and Hypotheses</a></div> <div><a href="#future-work">Future Work</a></div> </nav> </d-contents> <h2 id="project-proposal-overview">Project Proposal Overview</h2> <p>Welcome to our project proposal homepage! Below is an overview of what we’re interested in and how we plan on structuring our project, as well as some questions included at the bottom that we hope to get some advice/feedback/input on.</p> <h3 id="background">Background</h3> <ol> <li>Representation Primer <ul> <li>What is representation?</li> <li>Why is it important to learn well (properties of good representations and its utility)?</li> </ul> </li> <li>Autoencoder Primer <ul> <li>What is an autoencoder (AE) and how does it relate to representation?</li> </ul> </li> </ol> <h3 id="iterated-representation-learning-irl-framework">Iterated Representation Learning (IRL) Framework</h3> <ol> <li>AEs (deterministic reconstruction) <ul> <li>Step 1: Given some dataset, use an AE to learn its embedding space.</li> <li>Step 2: Using the learned embedding and AE, reconstruct the original dataset and compute the reconstruction loss.</li> <li>Step 3: Using the reconstructed dataset, repeat Steps 1 and 2, iterating as long as desired.</li> </ul> </li> <li>VAEs (generative modeling) <ul> <li>Step 1: Given some dataset, use a VAE to learn its embedding space.</li> <li>Step 2: Using the learned embedding and VAE, generate a new dataset.</li> <li>Step 3: Using the newly generated dataset, repeat Steps 1 and 2, iterating as long as desired.</li> </ul> </li> </ol> <h3 id="potential-questions-and-hypotheses">Potential Questions and Hypotheses</h3> <ol> <li>Following the iterated representation learning framework above, can we iterate until we reach some kind of convergence with respect to the model and/or learned embedding space? <ul> <li>If so, can this tell us any properties of the representation space, learned representation, model, and/or data?</li> <li>Does the number of iterations until convergence have anything to do with how “good” or stable the model or learned representation is?</li> </ul> </li> <li>In the deterministic autoencoder case, how do the reconstruction losses perform as iterations go on? Do we converge? How quickly? If the loss seems to diverge (relative to the original data), does it diverge linearly, exponentially, etc.?</li> <li>What can we say about characteristics of the data that are maintained through iterations, and characteristics that evolve as the iterations go on? <ul> <li>For example, if we observe that a model remains invariant to a certain feature, but becomes sensitive to new features of the data, what does this tell us about these particular features, our model, and the original data itself?</li> <li>Are there any other patterns we can identify along these lines?</li> </ul> </li> <li>Can we propose some sort of representation learning evaluation framework using iterated representation learning, e.g. rough guidelines on ideal number of iterations required until convergence, and what this says about how good a model is?</li> </ol> <h3 id="future-work">Future Work</h3> <ol> <li>How can we make iterated representation learning more computationally tractable?</li> <li>Can any of these results be generalized to other types of deep learning models?</li> <li>Are there any theoretical guarantees we can prove?</li> </ol> <h2 id="references-and-resources">References and Resources</h2> <h3 id="possible-data-sources">Possible Data Sources</h3> <ul> <li>MNIST, FashionMNIST</li> <li>CIFAR-10, CIFAR-100</li> <li>Pytorch’s Food101 dataset, CelebA dataset</li> <li>Tensorflow’s cats_vs_dogs dataset</li> </ul> <h3 id="possible-references">Possible References</h3> <ul> <li>Robustness of Unsupervised Learning Without Labels (Petrov and Kwiatkowska, 2022)</li> <li>Understanding Robust Learning through the Lens of Representation Similarities (Cianfarani et al., 2022)</li> <li>Using variational autoencoders to learn variations in data (Rudd and Wild, 2018)</li> </ul> <h2 id="questions-for-course-staff">Questions for Course Staff</h2> <ol> <li>Does this problem seem tractable, both theoretically and empirically?</li> <li>Our idea encompasses two analogous processes, a deterministic pipeline with reconstruction (using an AE), and a random pipeline with new data generation (using a VAE). Do you think either of these is more/less practical, feasible, or interesting to pursue?</li> <li>How would you recommend that we get started on this, beyond reading more existing literature on representation learning? We were thinking that perhaps we could try this approach on some smaller examples first (e.g. fixing a dataset and using a few different autoencoder models), and see if any interesting observations result from that, and then dive deeper based on those results. Any advice here would be greatly appreciated!</li> <li>Are there any theoretical components that you suggest we focus on, to potentially prove a small theoretical result?</li> <li>What empirical results/comparisons would you suggest us to be on the lookout for?</li> <li>Any other suggestions?</li> </ol> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/staging/assets/bibliography/2023-11-09-Regularization.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>