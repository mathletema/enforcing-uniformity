<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Project Proposal | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="A survey of various embeddings for spatio-temporal forecasting."/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/blog/2023/spatiotemporal/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/staging/assets/js/distillpub/template.v2.js"></script> <script src="/staging/assets/js/distillpub/transforms.v2.js"></script> <script src="/staging/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Project Proposal",
      "description": "A survey of various embeddings for spatio-temporal forecasting.",
      "published": "November 8, 2023",
      "authors": [
        {
          "author": "Joshua Sohn",
          "authorURL": "",
          "affiliations": [
            {
              "name": "MIT",
              "url": ""
            }
          ]
        },
        {
          "author": "Samuel Lee",
          "authorURL": "",
          "affiliations": [
            {
              "name": "MIT",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/staging/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Project Proposal</h1> <p>A survey of various embeddings for spatio-temporal forecasting.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#abstract">Abstract</a></div> <div><a href="#related-work">Related Work</a></div> <div><a href="#methodology">Methodology</a></div> <div><a href="#evaluation">Evaluation</a></div> </nav> </d-contents> <h2 id="abstract">Abstract</h2> <p>Time series forecasting is an interdisciplinary field that affects various domains, including finance and healthcare, where autoregressive modeling is used for informed decision-making. While many forecasting techniques focus solely on the temporal or spatial relationships within the input data, we have found that few use both. Our goal is to compare robust embeddings that capture both the spatial and temporal information inherent in datasets and possibly devise one ourselves. We will focus on the field of traffic congestion, which is a pervasive challenge in urban areas, leading to wasted time, increased fuel consumption, and environmental pollution. Accurate traffic flow forecasting is critical for traffic management, infrastructure planning, and the development of intelligent transportation systems. Through this project, we hope to discover the most effective method of generating spatiotemporal embeddings in traffic flow forecasting models.</p> <h2 id="related-work">Related Work</h2> <p>Currently, there are three different embedding techniques that we will be comparing in our project.</p> <p>The first is the Spatio-Temporal Adaptive Embedding transformer (STAEformer)<d-cite key="liu2023staeformer"></d-cite>. STAEformer uses adaptive embeddings, which adds an embedding layer on the input to dynamically generate learned embeddings on the dataset. In their architecture, the input embedding is then fed into temporal and spatial transformer layers, followed by a regression layer.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/staeformer_architecture-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/staeformer_architecture-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/staeformer_architecture-1400.webp"/> <img src="/staging/assets/img/2023-11-08-spatiotemporal/staeformer_architecture.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Architecture of the Spatio-Temporal Adaptive Embedding transformer (STAEformer).<d-cite key="liu2023staeformer"></d-cite> </div> <p>The second is the Spatio-Temporal Transformer with Relative Embeddings (STTRE)<d-cite key="deihim2023sttre"></d-cite>. STTRE uses relative position encodings, renamed as relative embeddings. The idea to leverage relative embeddings as a way to capture the spatial and temporal dependencies in the dataset of a multivariate time series. In their architecture, the relative embeddings are coupled with a transformer with multi-headed attention.</p> <p>The third is the Spacetimeformer<d-cite key="grigsby2023spacetimeformer"></d-cite>. Spacetimeformer uses embeddings generated from breaking down standard embeddings into elongated spatiotemporal sequences. In their architecture, these embeddings are fed into a variant of the transformer model using local, global, and cross self-attention.</p> <p>As the project progresses, we will continue looking for novel embeddings that have reached or are close to the sota benchmark in spatiotemporal forecasting and apply them to our model.</p> <h2 id="methodology">Methodology</h2> <p>In order to investigate the most effective method of generating spatiotemporal embeddings, we will standardize the rest of the architecture. After our embedding layer, we will build our own transformer model with a single spatiotemporal layer. This will be followed by a regression layer that outputs the prediction. We will keep these parts relatively simple to focus on the embedding layer, which is where we’ll incorporate the different techniques described in the related works section. We will also perform some ablation experiments to measure the efficacy of the methods used to generate the spatiotemporal embeddings</p> <p>To train and test our model, we will use traffic forecasting datasets that are available online. We are considering using the METR-LA dataset<d-cite key="metr-la"></d-cite> and the PEMS-BAY dataset<d-cite key="pems-bay"></d-cite> as they are popular choices in this field.</p> <p>If creating our own model seems infeasible, we will take an existing model and focus solely on the embedding layer. We’re currently settling on the STAEformer, as it outperformed the Spacetimeformer on the PEMS-BAY dataset when compared using the same performance metrics.</p> <h2 id="evaluation">Evaluation</h2> <p>We will be using common evaluation metrics in forecasting, such as MAE, MAPE, and MSE. We will also include the final accuracy of our model on the METR-LA and PEMS-BAY datasets.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/staging/assets/bibliography/2023-11-08-spatiotemporal.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>