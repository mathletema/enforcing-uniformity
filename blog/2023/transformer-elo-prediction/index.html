<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Transformer Based Chess Rating Prediction | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="Your blog post's abstract. This is an example of a distill-style blog post and the main elements it supports."/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/blog/2023/transformer-elo-prediction/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/staging/assets/js/distillpub/template.v2.js"></script> <script src="/staging/assets/js/distillpub/transforms.v2.js"></script> <script src="/staging/assets/js/distillpub/overrides.js"></script> </head> <d-front-matter> <script async type="text/json">{
      "title": "Transformer Based Chess Rating Prediction",
      "description": "Your blog post's abstract. This is an example of a distill-style blog post and the main elements it supports.",
      "published": "November 10, 2023",
      "authors": [
        {
          "author": "Anonymous",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/staging/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Transformer Based Chess Rating Prediction</h1> <p>Your blog post's abstract. This is an example of a distill-style blog post and the main elements it supports.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#proposal">Proposal</a></div> <ul> <li><a href="#data">Data</a></li> <li><a href="#methods">Methods</a></li> <li><a href="#evaluation">Evaluation</a></li> <li><a href="#relation-to-course-material">Relation to Course Material</a></li> </ul> </nav> </d-contents> <h2 id="proposal">Proposal</h2> <p>Motivated by a lack of concrete methods to estimate an unrated or unknown chess player’s skill, we present Transformer-Based Chess Rating Predictions. Our main idea is to train a transformer based architecture to predict the elo rating of chess players from the sequence of moves they make in a game.</p> <h3 id="data">Data</h3> <p>We can get data for games <a href="https://database.lichess.org/#standard_games">here</a>. For each game, we can consider the average rating of the players to be the thing we are trying to predict (we will only take games where players are within 400 rating points of each other). We may relax this restriction later on to include games with any rating gap, but we foresee difficulties in trying to disentangle the individual ratings in a given game. Our architecture is more suited to predicting the average rating between the two players, and the effect of differing playing styles may inject extra variance into rating predictions of individuals. We would be open to suggestions on how we could remedy this issue.</p> <h3 id="methods">Methods</h3> <p>One key decision we will have to make is on the best way to represent the data. Our current idea is to represent the game as a sequence of 3D Tensors, where each 2D “slice” represents some feature of the game state (positions of white pawns, castling rights, move repetitions, etc.). Crucially, we’ll also include the last move’s centipawn loss, which is a nonnegative measure of accuracy calculated by subtracting the engine evaluation of the played move from the engine evaluation of the engine-recommended move. Hopefully, this somewhat noisy notion of accuracy along with the context of the game state will provide enough information for the model to make accurate predictions.</p> <p>Our main architecture will consist of a transformer with an autoregressive attention mask. Each game state is fed through an initial linear layer to generate initial embeddings, after which they’re inputted into a transformer in which a token only attends on itself and tokens that come before it. The final layer consists of a linear layer that maps to a final rating prediction, which we will evaluate with MSE.</p> <h3 id="evaluation">Evaluation</h3> <p>To see if our transformer model is truly learning anything from the game states, we can compare our transformer-based model with a simpler baseline model: for example, an LSTM that predicts the same average rating where the only inputs are the moves’ centipawn losses. We would like our transformer’s MSE to be significantly lower than the LSTM’s MSE over our testing dataset.</p> <p>It would also be interesting to examine model behavior on “atypical” data - for example, on games with large rating gaps between two players or on tactically complex games in which even master-level players would make ample mistakes.</p> <h3 id="relation-to-course-material">Relation to Course Material</h3> <p>Our goal for this project is to improve our understanding of how to apply the more abstract concepts around transformers and input representation that we learned in class to a more concrete problem, and gain insight on what matters when optimizing the accuracy of our model (width vs depth of model, amount of data, diversity of data, amount of time to train, etc). Although we know the concepts behind what “should” improve accuracy, it would be interesting to see it play out in and the relative importance of different concepts (ex: perhaps, having a deeper model is not nearly as important as training for a long time).</p> <p>https://arxiv.org/pdf/1908.06660.pdf (can use a similar board representation)</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/staging/assets/bibliography/2023-11-10-transformer-elo-prediction.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>