<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>How to learn a linear representation of a dynamical system | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="A final project proposal for 6.s898 in fall 2023"/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/blog/2023/croneillproposal/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/staging/assets/js/distillpub/template.v2.js"></script> <script src="/staging/assets/js/distillpub/transforms.v2.js"></script> <script src="/staging/assets/js/distillpub/overrides.js"></script> </head> <d-front-matter> <script async type="text/json">{
      "title": "How to learn a linear representation of a dynamical system",
      "description": "A final project proposal for 6.s898 in fall 2023",
      "published": "November 8, 2023",
      "authors": [
        {
          "author": "Cormac O'Neill",
          "authorURL": "",
          "affiliations": [
            {
              "name": "MIT, Cambridge",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/staging/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>How to learn a linear representation of a dynamical system</h1> <p>A final project proposal for 6.s898 in fall 2023</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#proposal">Proposal</a></div> </nav> </d-contents> <h2 id="proposal">Proposal</h2> <p>Linear system representations offer numerous benefits for analysis and control. Unfortunately, we live in a world where most interesting dynamic systems are inherently nonlinear. Traditionally engineers have linearized nonlinear systems by truncating a Taylor series approximation of the dynamics about a point. While this technique can be useful, it is an inherently point-wise approach. In contrast, recent work has investigated how lifting linearization techniques can be used as an alternative. Underpinned by Koopman operator theory, lifting linearization expands a nonlinear system to a higher dimension by appending nonlinear functions of its state to the system’s representation <d-cite key="brunton2021modern"></d-cite>. One of the primary open questions in the field is how to best select these nonlinear functions (referred to as “observable functions”). A recent, popular approach is to learn the observable functions from data with a neural network <d-cite key="lusch2018deep, yeung2019learning, abraham2019active, han2020deep"></d-cite>. This network usually takes on the form of an autoencoder with a representation space that is a higher dimension than the input.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-croneillproposal/deepnet-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-croneillproposal/deepnet-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-croneillproposal/deepnet-1400.webp"/> <img src="/staging/assets/img/2023-11-08-croneillproposal/deepnet.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> An example of a neural network architectured used to learn observables for a linear Koopman model, taken from <d-cite key="lusch2018deep"></d-cite> </div> <p>For this project, I want to investigate how deep learning can be used to learn more effective observable functions. I am especially interested in studying how to learn observables for piecewise dynamical systems:</p> <ul> <li> <p>Can a curriculum learning-inspired approach lead to observables with varying spatial frequencies? Can we first train an autoencoder to learn a small number of observables that are better at representing the system’s averaged global dynamics at the expense of local accuracy? If we then add additional width to the network and continue training, will we be able to learn observables that are more focused on particular regions of state space?</p> </li> <li> <p>If observables are separately trained on different regions of state space, can they then be concatenated to provide a better dynamic model? This approach is inspired by work from a previous lab mate of mine <d-cite key="ng2022learned"></d-cite>.</p> </li> </ul> <p>I plan to take an ablative approach to studying these questions by training three different models: a standard network for learning observables that works on the full training data set, the above curriculum learning approach, and then finally an approach that uses observables trained separately on different regions of state space. I will then compare the performance of the resulting observables in predicting the trajectory of a dynamical system.</p> <p>I am also considering some additional questions that could be interesting, although they are less well thought out:</p> <ul> <li> <p>How can the autoencoder structure of observable generators be modified to improve performance? I need to do further literature review, but I do not believe that there has been a quantitative analysis of how network architecture (such as the type of activation function, the importance of depth) impacts performance. I am not even sure if skip connections have been utilized in prior work.</p> </li> <li> <p>Are there alternatives to fully-connected layers that could be useful for generating observable functions? I have given this question much less thought, but it is a topic I would love to discuss with the TAs. Certain lifted linearization approaches (dynamic mode decomposition) work by taking measurements throughout the state space and using them as observables. For example, a highly nonlinear fluid flow can be linearized by taking measurements throughout the fluid. This creates a data structure that reminds me of images, causing me to wonder if a convolutional or transformer inspired approach could have some use in this field.</p> </li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/staging/assets/bibliography/2023-11-08-croneillproposal.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>