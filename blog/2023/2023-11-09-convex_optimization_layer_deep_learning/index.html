<h2 id="convex-optimization-as-a-layer-in-neural-network-architectures">Convex Optimization as a Layer in Neural Network Architectures</h2> <p>Convex optimization is a well-studied area of operations research. There has recently been an insurgence of work relating the field to machine learning. Agrawal et al. <d-cite key="agrawal2019differentiable"></d-cite> propose a method known as ``disciplined parameterized programming’’, which maps the parameters of a given convex program to its solution in a differentiable manner. This allows us to view instances of convex optimization programs as functions mapping problem-specific data (i.e input) to an optimal solution (i.e output). For this reason, we can interpret a convex program as a differentiable layer with no trainable parameters in the same way as we think of ReLU as a layer in a deep neural network. Past work (<d-cite key="amos2021optnet"></d-cite>, <d-cite key="barratt2019differentiability"></d-cite>) has primarily focused on providing methods for differentiability of the convex optimization layer. However, an unexplored question remains: for which types of machine learning problems does this architecture provide an edge over other architectures?</p> <h2 id="the-role-of-convex-optimization-layers-for-various-machine-learning-tasks">The Role of Convex Optimization Layers for Various Machine Learning Tasks</h2> <p>We hypothesize that architectures which leverage convex optimization layers may perform better on some machine learning tasks than others. CNNs have become the gold standard for solving supervised learning prediction tasks from image data. Transformers are now the go-to architecture in generative modeling when working with language. However, it remains unclear in which settings, if any, we may rely on convex optimization layers as the default choice of architecture.</p> <p>This project explores when such an architecture might be well-suited in machine learning. Specifically, we will implement a disciplined parametrized program for three separate tasks, in very different types of machine learning problems. We will then compare the performance of convex optimization as a layer between these tasks, using various metrics and baselines. This will provide insight as to which machine learning tasks are best suited for architectures with convex optimization layers.</p> <h3 id="supervised-learning">Supervised Learning</h3> <p>We consider the supervised learning problem of predicting the solution to a sudoku puzzle from its image representation. We will compare against baseline CNN or MLP models, and we will compare the accuracy and amount of training needed across these architectures. We will render solutions to sudoku puzzles in the context of convex optimization, and we hypothesize that the inductive bias of our architecture will provide better performance from existing architectures. In particular, we hypothesize that convex optimization as a layer will require less training and higher accuracy than for MLP and CNN architectures.</p> <h3 id="reinforcement-learning">Reinforcement Learning</h3> <p>We consider the control problem of steering a car above a hill, otherwise known as MountainCar, from the OpenAI gym benchmark of RL environments. We can model the problem with quadratic reward, and linear transition function, so that the optimal controller would be quadratic in state. By contextualizing the action as a solution to a convex optimization problem, we can enforce safety constraints explicitly, for stability of training of the agent. We will compare this model against baseline RL algorithms such as PPO, and will compare standard RL metrics, such as mean reward.</p> <h3 id="generative-modeling">Generative Modeling</h3> <p>We consider the generative learning problem of sampling maps for atari video games which satisfy specific conditions, such as the location of blocks or coins. We can make the data samples be solutions to an optimization problem, which enforces certain constraints on the generated solution, such as the locations or colors of features in the game. Then, by adding noise, and predicting the mean of noisy samples, we can generate fresh valid configurations also satisfying our optimization constraints. We will test the accuracy of our architecture by testing its accuracy across various tests and environments.</p>