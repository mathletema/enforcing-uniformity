<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Unraveling Social Reasoning in LLMs - A Decision Tree Framework for Error Categorization | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="In this study, we investigate the challenge of social commonsense reasoning in large language models (LLMs), aiming to understand and categorize common errors LLMs make in social commonsense reasoning tasks. Our approach involves expanding upon the preliminary qualitative analyses of social reasoning errors, then developing a decision tree framework for more nuanced and fine-grained error categorization. We will test models such as GPT using this framework. We expect to better understand error types and themes in LLMs' social reasoning, offering insights for improving their performance in understanding complex social interactions."/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/blog/2023/unraveling-social-reasoning-in-llms/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/staging/assets/js/distillpub/template.v2.js"></script> <script src="/staging/assets/js/distillpub/transforms.v2.js"></script> <script src="/staging/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Unraveling Social Reasoning in LLMs - A Decision Tree Framework for Error Categorization",
      "description": "In this study, we investigate the challenge of social commonsense reasoning in large language models (LLMs), aiming to understand and categorize common errors LLMs make in social commonsense reasoning tasks. Our approach involves expanding upon the preliminary qualitative analyses of social reasoning errors, then developing a decision tree framework for more nuanced and fine-grained error categorization. We will test models such as GPT using this framework. We expect to better understand error types and themes in LLMs' social reasoning, offering insights for improving their performance in understanding complex social interactions.",
      "published": "November 11, 2023",
      "authors": [
        {
          "author": "Nina Lei",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Harvard College",
              "url": ""
            }
          ]
        },
        {
          "author": "Andrew Zhao",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Harvard College",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/staging/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Unraveling Social Reasoning in LLMs - A Decision Tree Framework for Error Categorization</h1> <p>In this study, we investigate the challenge of social commonsense reasoning in large language models (LLMs), aiming to understand and categorize common errors LLMs make in social commonsense reasoning tasks. Our approach involves expanding upon the preliminary qualitative analyses of social reasoning errors, then developing a decision tree framework for more nuanced and fine-grained error categorization. We will test models such as GPT using this framework. We expect to better understand error types and themes in LLMs' social reasoning, offering insights for improving their performance in understanding complex social interactions.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#research-questions">Research Questions</a></div> <ul> <li><a href="#rq1">RQ1</a></li> <li><a href="#experimental-setup">Experimental Setup</a></li> <li><a href="#rq2">RQ2</a></li> </ul><div><a href="#methodology">Methodology</a></div> <div><a href="#expected-outcomes">Expected Outcomes</a></div> <div><a href="#references">References</a></div> </nav> </d-contents> <h1 id="unraveling-social-reasoning-in-llms-a-decision-tree-framework-for-error-categorization">Unraveling Social Reasoning in LLMs: A Decision Tree Framework for Error Categorization</h1> <h2 id="introduction">Introduction</h2> <p>Despite recent advances and the growth in scale of large language models (LLMs), it’s unclear how capable models are of reasoning, especially social commonsense reasoning. <d-cite key="huang_towards_2023"></d-cite> Tasks involving navigating complex social norms, emotions, and interactions remain a developing frontier in LLM research.</p> <p>Prior works like SOCIAL IQA <d-cite key="sap_socialiqa_2019"></d-cite>, ProtoQA, <d-cite key="boratko_protoqa_2020"></d-cite>, Understanding Social Reasoning in Language Models with Language Models <d-cite key="gandhi_understanding_2023"></d-cite> , and SOCIAL CHEMISTRY 101 have provided benchmarking datasets and techniques for social commonsense reasoning and social norms <d-cite key="forbes_social_2021"></d-cite>. Other works, such as Neural Theory-Of-Mind <d-cite key="sap_neural_2023"></d-cite>, explore why models struggle on these datasets and/or try to improve performance, such as by using knowledge graphs. <d-cite key="li_systematic_2022"></d-cite> <d-cite key="chang_incorporating_2020"></d-cite></p> <p>Therefore, our research has two goals: firstly, to expand upon previous research about the types of errors that LLMs make on social reasoning tasks, and secondly, to devise new categories that allow for better granularity when interpreting these mistakes that can help with finetuning models on these errors.</p> <h2 id="research-questions"><strong>Research Questions</strong></h2> <ul> <li>RQ1: What are underlying themes in social errors that large language models make?</li> <li>RQ2: Are there methods that could potentially address these errors?</li> </ul> <h2 id="methodology"><strong>Methodology</strong></h2> <h3 id="rq1"><strong>RQ1</strong></h3> <ol> <li> <p><strong>Preliminary Qualitative Analysis:</strong></p> <p>We will build upon benchmarking datasets based on Social IQA <d-cite key="sap_socialiqa_2019"></d-cite> and other datasets that provide categorization of social knowledge. An instance of such benchmark is the Social IQA Category dataset <d-cite key="wang_semantic_2021"></d-cite>, which considers four social knowledge categories: Feelings and Characteristics; Interaction; Daily Events; and Knowledge, Norm, and Rules. The authors of this paper found that RoBERTa-large performed the worst in the Feelings and Characteristics category, but did not provide general qualitative or quantitative observations about the types of errors made in each category. We want to better understand these sorts of errors made by the model in these domains.</p> <p>We plan to conduct an initial qualitative analysis to determine themes in common errors made in each of the four categories. For each model, we plan on sampling 20 or more questions in which the model does not answer correctly, then performing standard qualitative coding procedures to identify a set of common themes in errors for each category.</p> <p>Beyond testing the models listed in previous papers, we would like to explore how good GPT-4 is at answering these social commonsense reasoning questions. Given GPT-4’s improved capabilities compared to GPT-3, we suspect that this model will perform better; assessing its performance would allow other researchers to draw different insights into how architecture changes and expansions affect social reasoning.</p> </li> <li> <p><strong>Refinement and Analysis</strong></p> <p>Based on the insights gained from the preliminary qualitative analysis, we plan on devising more specific social knowledge categories than the four considered by the Social IQA Category dataset; we aim to construct categories based off of building a decision tree abstraction, where each dimension in the tree corresponds to a trait about the question.</p> <p>An example set of dimensions for the decision tree abstraction is as follows:</p> <ul> <li>Dimension 1: Social IQA Category’s four social knowledge categories</li> <li>Dimension 2: Type of question (effects, pre-conditions, stative descriptions, etc.) <ul> <li>Note: The authors in the original SocialIQA paper noted that BERT-large found questions related to “effects” to be the easiest and questions about “descriptions” to be the most challenging and claimed that models found stative questions difficult.</li> </ul> </li> <li>Dimension 3: Whether reasoning “is about the main agent of the situation versus others.” <ul> <li>In Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs, the authors argued that models perform much worse when the question is not about the main agent of the situation.</li> </ul> </li> </ul> <p>The goal here is to offer a more granular understanding of the categories of errors LLMs make on social reasoning questions. We will then perform another round of Preliminary Qualitative Analysis assessing themes in errors in each category to see if our categories improve on other papers’ categories.</p> </li> </ol> <p><strong>Experiment Setup</strong></p> <ul> <li>We will qualitatively assign each example under some combination of categories, trying to recreate the purported low performance on these social-reasoning datasets.</li> <li>Due to constraints in time, access, and computational resources, we plan on probing models like GPT-3 through through an API, similar to how it was done in the paper Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs. <ul> <li>Specifically, we will test on GPT-4 to see if these shortcomings still apply.</li> </ul> </li> </ul> <h3 id="rq2"><strong>RQ2</strong></h3> <ul> <li>What we do here is largely dependent upon RQ1 findings; strategies for addressing errors in social commonsense reasoning are largely contingent on the types and themes of errors identified in RQ1.</li> <li>We also consider existing approaches to enhancing capabilities of LLMs when it comes to social commonsense reasoning. Namely, in literature, many papers have experimented with the integration of external knowledge bases and fine-tuning models with semantic categorizations of social knowledge.</li> </ul> <h2 id="expected-outcomes">Expected Outcomes</h2> <ul> <li>A comprehensive catalog of error types and themes in LLMs concerning social reasoning.</li> <li>Insights into the comparative analysis of different models on social reasoning benchmarks.</li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/staging/assets/bibliography/2022-12-01-distill-example.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>