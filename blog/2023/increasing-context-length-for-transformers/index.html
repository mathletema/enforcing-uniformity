<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>6.S898 Project Proposal | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="Increasing Context Length For Transformers"/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/blog/2023/increasing-context-length-for-transformers/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/staging/assets/js/distillpub/template.v2.js"></script> <script src="/staging/assets/js/distillpub/transforms.v2.js"></script> <script src="/staging/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "6.S898 Project Proposal",
      "description": "Increasing Context Length For Transformers",
      "published": "November 8, 2023",
      "authors": [
        {
          "author": "Annie Wang",
          "authorURL": "",
          "affiliations": [
            {
              "name": "MIT",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/staging/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>6.S898 Project Proposal</h1> <p>Increasing Context Length For Transformers</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#overview">Overview</a></div> </nav> </d-contents> <h2 id="overview">Overview</h2> <p>Modern-day transformers often aim to solve problems that utilize large inputs with long-range dependencies. For instance, the development of sophisticated LLMs such as GPT-4 has given rise to text prompts that are several hundred words in length, where the first sentence may impact the interpretation of the last. Today’s transformers—particularly LLMs—often come with a maximum context length, so that excessively long inputs are not accepted. Yet, this context length is often exceeded when trying to solve complex problems within the model’s domain; as an example, we consider the task of summarizing long documents via GPT-4.</p> <p>Evidently, a transformer’s maximum context length greatly affects the types of information it can process and the questions it can answer; larger context lengths would allow transformers to solve even more complex problems.</p> <p>However, the time complexity of transformers is quadratic with regard to the input length. In the traditional transformer model, each element in the input sequence is mapped to one or more tokens, and each token attends to every token prior to it—making the attention mechanism a relatively expensive computation. As a result, strategies for decreasing the context length of large inputs is a very relevant topic in the development of transformers.</p> <p>In this project, we investigate the effects of large context length on transformers, along with current methods of increasing context length. Additionally, we evaluate the advantages and disadvantages of current approaches for increasing context length and attempt to apply them to different transformer-based problems. Finally, we propose a new scheme for increasing context length. We test this scheme via ablation studies and aim to explain why or why not it does not perform as well as current approaches.</p> <p>A more detailed breakdown of the project plan is provided below.</p> <table> <thead> <tr> <th>Task</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>Investigate effects of increasing context length without limiting the number of tokens that must be attended upon.</td> <td>Train transformers to solve the same problem (e.g., language generation based on a provided dataset), but with different maximum context lengths. Assess the performance of the resulting models, including how well they are able to solve the initial problem and how long they take to train and generate data.</td> </tr> <tr> <td>Survey current approaches for increasing context length.</td> <td>Investigate how current approaches aim to increase context length while reducing overall time complexity. Discuss different advantages and disadvantages of current methods.</td> </tr> <tr> <td>Assess advantages and disadvantages of current approaches for increasing context length, as applied to specific transformer-based problems.</td> <td>Investigate whether certain methods for reducing context length work better for certain problems than for others. Why or why not? Investigate whether proposed methods work as well in practice as they do in theory.</td> </tr> <tr> <td>Investigate a new scheme for increasing context length.</td> <td>Using existing knowledge, propose a new scheme for increasing context length and provide an explanation as to why the selected scheme was chosen.</td> </tr> <tr> <td>Test the proposed scheme for increasing context length.</td> <td>Attempt to solve an existing transformer-based problem using the new scheme. Compare results to results using existing approaches. Provide a hypothesis as to why the new scheme works or does not work.</td> </tr> </tbody> </table> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/staging/assets/bibliography/2023-11-08-increasing-context-length-for-transformers.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>