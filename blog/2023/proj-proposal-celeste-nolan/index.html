<hr/> <p>layout: distill title: 6.S898 Project Proposal description: t date: 2023-11-09 htmlwidgets: true</p> <p>authors:</p> <ul> <li>name: Carlos Celeste Jr. url: “celeste8@mit.edu” affiliations: name: MIT</li> <li>name: Nick Nolan url: “ncn@mit.edu” affiliations: name: MIT</li> </ul> <h2 id="project-proposal">Project Proposal</h2> <p>The study of biological systems with machine learning is a burgeoning field; however, within some subfields of study, gathering sufficient data to train a model is a significant roadblock. For example, rigorously characterizing the in vitro performance of synthetic biological circuits is taxing on both a researcher’s budget and time — a single experiment may take upwards of 12 hours of attentive action, while yielding only up to 96 data points for training. This necessitates the consideration of alternative methods by which to reduce the quantity of data needed to train an effective model, or develop more efficient methods by which to produce more data. To this end, there are many mathematical models with varying degrees of complexity that capture key qualitative and/or quantitative behaviors from biological systems, which could be used to generate synthetic data. However, these models are not perfect: even these most complex models fail to encapsulate the full depth of a cell’s context.</p> <p>With this in mind, this project will investigate the use of transfer learning to reduce the number of datapoints from “experiments” (for our project, we will use the aforementioned complex models as a stand-in for actual experimental data) by pre-training the neural network with a simple model first. Moreover, the project will focus on how the different synthetic data distributions generated by the models affect the neural network and aim to determine the necessary assumptions on these distributions such that transfer learning is possible.</p> <p>To this end, three biological models will be considered: a simple resource sharing model, a complex resource sharing model (which will represent the experimental data), and an activation cascade model, which will represent the experimental data from a fundamentally different biological system. A big dataset from the simple resource sharing model will be used for pre-training an multilayer perceptron (MLP) and then a small dataset from the complex resource sharing model will be used to complete the MLP training, which will be compared to another MLP that was trained using only a big dataset from the complex model. Furthermore, the same process will be repeated but with a small dataset from the activation cascade model to explore if transfer learning can be used across different models.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/fig1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/fig1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/fig1-1400.webp"/> <img src="/staging/assets/img/fig1.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The three biological models that we will be considering. One, in which a Resource R1 affects our two outputs X1 and X2; another, in which our Resource R1 comes together with a second copy of itself to form a secondary Resource R2, which serves the same function as the R1 from before; and a final one, in which the outputs X1 and X2 are directly correlated, but there are no resources to consider. </div> <p>In addition to these comparisons, an exploration of the effects of each dataset on the MLP will be conducted with the goal of identifying the key similarities and differences in the datasets that may lead to success or failure to transfer learning between them.</p>