<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Graph neural networks v.s. transformers for geometric graphs | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="With the recent development of graph transformers, in this project we aim to compare their performance on a molecular task of protein-ligand binding affinity prediction against the performance of message passing graph neural networks."/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/blog/2023/proposal/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/staging/assets/js/distillpub/template.v2.js"></script> <script src="/staging/assets/js/distillpub/transforms.v2.js"></script> <script src="/staging/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Graph neural networks v.s. transformers for geometric graphs",
      "description": "With the recent development of graph transformers, in this project we aim to compare their performance on a molecular task of protein-ligand binding affinity prediction against the performance of message passing graph neural networks.",
      "published": "November 1, 2023",
      "authors": [
        {
          "author": "Ada Fang",
          "authorURL": "",
          "affiliations": [
            {
              "name": "MIT",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/staging/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Graph neural networks v.s. transformers for geometric graphs</h1> <p>With the recent development of graph transformers, in this project we aim to compare their performance on a molecular task of protein-ligand binding affinity prediction against the performance of message passing graph neural networks.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#relevant-work">Relevant work</a></div> <ul> <li><a href="#graph-neural-networks">Graph neural networks</a></li> <li><a href="#graph-transformers">Graph transformers</a></li> </ul><div><a href="#problem-definition">Problem definition</a></div> <div><a href="#dataset">Dataset</a></div> <div><a href="#proposed-experiments">Proposed experiments</a></div> <ul> <li><a href="#proposed-algorithmic-contributions">Proposed algorithmic contributions</a></li> <li><a href="#can-transformers-better-capture-long-range-interactions">Can transformers better capture long range interactions</a></li> <li><a href="#can-graph-neural-networks-approximate-transformers-with-a-fully-connected-graph">Can graph neural networks approximate transformers with a fully connected graph</a></li> </ul> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Machine learning on graphs is often approached with message passing graph neural network (GNN) models where nodes in the graph are embedded with aggregated messages passed from neighboring nodes <d-cite key="zhou2020graph"></d-cite>. However, with the significant success of transformers in language modelling <d-cite key="vaswani2017attention"></d-cite> and computer vision recently <d-cite key="dosovitskiy2020image"></d-cite>, there are a growing number of transformers developed for graphs as well. In this project we investigate the application of graph neural networks compared to transformers on geometric graphs defined on point clouds. We aim to explore the performance of these two models on predicting the binding affinity for a protein-ligand interaction given the atomic coordinates of the docked protein-ligand structure, which is a highly relevant task in drug discovery.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-proposal/protein-ligand-structure-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-proposal/protein-ligand-structure-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-proposal/protein-ligand-structure-1400.webp"/> <img src="/staging/assets/img/2023-11-09-proposal/protein-ligand-structure.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A protein-ligand structure, Protein Data Bank (PDB) entry 1a0q. The protein backbone is shown in blue, and the ligand is shown in green. The model would be given this structure and the objective is to predict the binding affinity of the ligand to the protein. </div> <h2 id="relevant-work">Relevant work</h2> <p>Early applications of machine learning on molecules were mainly with graph neural networks. However, with the proliferation of transformers in the machine learning field, this has also influenced the development of graph transformers. Here we summarise a few key contributions in these two model archetypes for molecules.</p> <h3 id="graph-neural-networks">Graph neural networks</h3> <p>Here we focus on some key works on SE(3)-equivariant graph neural networks–where model outputs transform in the same way as inputs under 3D global translations and rotations–which are effective for modelling geometric data. Early graph neural networks on point clouds which used directional message passing <d-cite key="gasteiger2020directional"></d-cite> were limited in expressivity <d-cite key="garg2020generalization"></d-cite>. Now state-of-the-art (SOTA) models in this area are based on higher order geometric properties such as dihedral angles and representations in the geometric group SO(3). Some examples include GemNet <d-cite key="gasteiger2021gemnet"></d-cite> and e3nn <d-cite key="geiger2022e3nn"></d-cite>. These models have led to exceptional performance for tasks related to predicting molecular forces and energies <d-cite key="batzner20223"></d-cite> <d-cite key="musaelian2023learning"></d-cite>. For the task of binding affinity some models that achieve high performance using GNNs are from the following papers <d-cite key="wang2022learning"></d-cite> <d-cite key="somnath2021multi"></d-cite>.</p> <h3 id="graph-transformers">Graph transformers</h3> <p>Graph transformers have also been applied to molecules for property prediction. Graph transformers and sequence transformers are largely similar in architecture; however, differences arise in the positional encodings in a graph transformer as it is defined in relation to other nodes in the graph <d-cite key="ying2021transformers"></d-cite>. For geometric graphs, positional encodings can be applied as a bias term on the attention value of node $u$ on $v$, where the bias is a learned value that is dependent on the distance between the nodes <d-cite key="zhou2023uni"></d-cite> <d-cite key="luo2022one"></d-cite>. There are also other ways of implementing positional encodings in the form of Laplacian eigenvectors, and random walk diagonals <d-cite key="rampavsek2022recipe"></d-cite>. Recently, in an effort to unify different methods to generate structural and positional graph encodings, Liu et al. <d-cite key="liu2023graph"></d-cite> apply a novel pretraining approach with a multiobjective task of learning a variety of positional and structural encodings to derive more general positional and structural encodings. Graph transformers are also achieving SOTA performance for benchmarks on predicting quantum properties of molecules <d-cite key="zhou2023uni"></d-cite> <d-cite key="luo2022one"></d-cite> and binding affinity <d-cite key="kong2023generalist"></d-cite>.</p> <h2 id="problem-definition">Problem definition</h2> <p>The input to the model is a set of atoms for the protein $X_{\mathrm{protein}}$ and ligand $X_{\mathrm{ligand}}$, for which we have the atomic identity and the 3D coordinates, and the binding affinity $y$ for the structure. For the graph neural network we define a molecular graph of the protein ligand structure $G=(V,E)$ where $V$ are the $n$ nodes that represent atoms in the molecule and the edges $E$ are defined between two nodes if their 3D distance is within a radial cutoff $r$. For the graph transformer it is applied to the whole set of atoms $(X_{\mathrm{protein}}, X_{\mathrm{ligand}})$, and we can use the 3D coordinates of the atoms to derive positional encodings. Performance is determined by the root mean squared error, Pearson, and Spearman correlation coefficients between true binding affinity and predicted binding affinity.</p> <h2 id="dataset">Dataset</h2> <p>We use the PDBbind dataset for the protein-ligand structures and binding affinity. In addition, for benchmarking we use the benchmark from ATOM3D <d-cite key="townshend2020atom3d"></d-cite> with a 30% and 60% sequence identity split on the protein to better test generalisability of the model.</p> <h2 id="proposed-experiments">Proposed experiments</h2> <p>We will implement two models, a SE(3)-equivariant graph neural network based on Tensor Field Networks using e3nn <d-cite key="geiger2022e3nn"></d-cite> and DiffDock <d-cite key="corso2022diffdock"></d-cite> (a protein-ligand docking model), and a graph transformer based on the architecture proposed by Transformer-M <d-cite key="luo2022one"></d-cite>. For fair comparison we will ensure the number of trainable parameters in both models is comparable by adjusting the number of layers and embedding dimension. The models will be trained to convergence on the ATOM3D dataset split and the best performing model on the validation split will be used to evaluate the test split.</p> <h3 id="proposed-algorithmic-contributions">Proposed algorithmic contributions</h3> <p>For the GNN we will use the confidence model in DiffDock <d-cite key="corso2022diffdock"></d-cite> as an analogy to the binding affinity predictor model. The confidence model in DiffDock is given a docked protein-ligand structure and it scores how likely the structure is within 2 $\overset{\circ}{A}$ to the true structure. Similarly, the binding affinity model will be given the coordinates of the experimental protein-ligand structure and will predict the protein-ligand binding affinity.</p> <p>For the transformer, Transformer-M <d-cite key="luo2022one"></d-cite> is pretrained on a broad set of 2D and 3D molecular structures and has been finetuned to predict protein-ligand binding affinity. However, we would like to compare this to a GNN model in a fair way, which would require using the Transformer-M architecture for only the 3D structure input track and predicting binding affinity with only the training dataset.</p> <h3 id="can-transformers-better-capture-long-range-interactions">Can transformers better capture long range interactions</h3> <p>Fundamentally, transformers vary from graph neural networks with their ability to capture long range interactions compared to the $k$-hop neighbourhoods that can be captured by a $k$-layer graph neural network. We explore how model performance is a function of graph size and diameter for the two model archetypes to see if transformers are better at capturing long range interactions. We will also isolate subsets of molecules where the models achieve the best and worse performance to compare if the models are excelling in similar areas.</p> <h3 id="can-graph-neural-networks-approximate-transformers-with-a-fully-connected-graph">Can graph neural networks approximate transformers with a fully connected graph</h3> <p>One of the fundamental differences between transformers and GNNs is the neighborhood of nodes that each node receives updates from. For a transformer this is all nodes in a graph, and for a GNN this is the $k$-hop neighborhood. To bridge these differences we can construct a fully connected graph by increasing the radial cutoff $r$ for edges in the graph. We want to test for a GNN trained on a fully connected graph if we would achieve similar performance to the graph transformer.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/staging/assets/bibliography/2023-11-09-proposal.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>