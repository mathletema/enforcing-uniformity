<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Reasoning with Maps: Assessing Spatial Comprehension on Maps in Pre-trained Models | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="Assessing Spatial Comprehension on Maps in Pre-trained Models"/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/blog/2023/mapreason/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/staging/assets/js/distillpub/template.v2.js"></script> <script src="/staging/assets/js/distillpub/transforms.v2.js"></script> <script src="/staging/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Reasoning with Maps: Assessing Spatial Comprehension on Maps in Pre-trained Models",
      "description": "Assessing Spatial Comprehension on Maps in Pre-trained Models",
      "published": "November 8, 2023",
      "authors": [
        {
          "author": "Abdulrahman Alabdulkareem (arkareem@mit.edu)",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        },
        {
          "author": "Meshal Alharbi (meshal@mit.edu)",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/staging/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Reasoning with Maps: Assessing Spatial Comprehension on Maps in Pre-trained Models</h1> <p>Assessing Spatial Comprehension on Maps in Pre-trained Models</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#motivation">Motivation</a></div> <div><a href="#project-outline">Project Outline</a></div> <div><a href="#benchmark-dataset">Benchmark & Dataset</a></div> <div><a href="#black-box">Black Box</a></div> <div><a href="#investigating-representation">Investigating representation</a></div> <ul> <li><a href="#representation">Representation</a></li> <li><a href="#generation">Generation</a></li> </ul> </nav> </d-contents> <h1 id="motivation">Motivation:</h1> <p>Humans possess a remarkable ability to intuitively understand and make sense of maps, demonstrating a fundamental capacity for spatial reasoning, even without specific domain knowledge. To illustrate this, consider the following question: Do these two maps represent the same location?</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/Picture2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/Picture2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/Picture2-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/Picture2.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/Picture3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/Picture3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/Picture3-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/Picture3.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Answering this query necessitates coregistration, the ability to align two maps by overlaying their significant landmarks or key features. Moreover, humans can go beyond mere alignment; they can tackle complex inquiries that demand aligning maps, extracting pertinent data from each, and integrating this information to provide answers.</p> <p>Now, do contemporary state-of-the-art (SOTA) machine learning models, pre-trained on vast datasets comprising millions or even billions of images, possess a similar capacity for spatial reasoning? This project is dedicated to probing this question.</p> <h1 id="project-outline">Project Outline:</h1> <p>There are three main stages in this project.</p> <h2 id="benchmark--dataset">Benchmark &amp; Dataset:</h2> <p>After conducting a brief review of existing literature, we observed a lack of established benchmarks that could effectively evaluate the central question of our project. As a result, we plan to start this project by constructing a simple dataset/benchmark tailored for assessing map comprehension (e.g., coregistration). Our data collection will be sourced from the aviation domain, where a single location is usually depicted in multiple maps, each with distinct styles and information content (like the images shown above).</p> <p>Furthermore, as a baseline, we are considering assessing category recognition without involving spatial reasoning. As an illustration, we use a dataset with images of cats and dogs rendered in various artistic styles, where the model’s task is to determine whether two images belong to the same category or different categories.</p> <h2 id="black-box">Black Box:</h2> <p>Treating a pre-trained model as a black box. The first question we plan to investigate is if SOTA multimodal models are already capable (i.e., zero-shot testing without any fine-tuning) of this form of spatial reasoning. For example, models like GPT4V, Clip, VisualBERT, and many others. We anticipate that the answer will likely be negative, especially for complex queries like the following:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/Picture4-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/Picture4-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/Picture4-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/Picture4.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/Picture5-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/Picture5-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/Picture5-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/Picture5.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>“What is the name of the waypoint on top of Erie County?”</p> <p>This query would require first identifying the landmarks in each map separately, then aligning the two maps using a shared landmark (“Cory Lawrence” or the shoreline in this example), then finding the location of what is being asked (“Erie County” in the second image in this example), then transform that point to the first map using the established mapping, then finally find and return the name of the waypoint as the answer (“WABOR” in this example).</p> <h2 id="investigating-representation">Investigating representation:</h2> <p>Investigating the representation/embedding of a pre-trained model. If current models prove to be incapable or inadequate in terms of spatial reasoning capabilities, we plan to investigate why this might be the case by examining their internal representations through multiple approaches:</p> <h3 id="representation">Representation:</h3> <p>We will compute the embedding using SOTA CLIP models available then linearly probe the embedding to see if they can solve our task (i.e., few-shot learning on CLIP representation).</p> <h3 id="generation">Generation:</h3> <p>Can we generate maps from the embedding of CLIP models to learn more about what details they capture and what they fail to capture?</p> <ul> <li>Use zero-shot image generation to go from clip embeddings to images.</li> <li>Or fine-tune a model using parameter efficient tuning (e.g., ControlNet) to better generate images that match our task.</li> </ul> <p>##########################################################################</p> <p>https://skyvector.com/</p> <p>https://www.faa.gov/air_traffic/flight_info/aeronav/digital_products/</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/staging/assets/bibliography/2023-11-08-mapreason.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>