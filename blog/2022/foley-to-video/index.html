<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Foley-to-video generating video from environmental audio | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="In this blog we will explore the optimal architecture for generating video from environmental sound inputs."/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/blog/2022/foley-to-video/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/staging/assets/js/distillpub/template.v2.js"></script> <script src="/staging/assets/js/distillpub/transforms.v2.js"></script> <script src="/staging/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Foley-to-video generating video from environmental audio",
      "description": "In this blog we will explore the optimal architecture for generating video from environmental sound inputs.",
      "published": "November 8, 2022",
      "authors": [
        {
          "author": "Esteban Ramirez Echavarria",
          "authorURL": "https://www.linkedin.com/in/esteban-raech/",
          "affiliations": [
            {
              "name": "LGO, MIT",
              "url": ""
            }
          ]
        },
        {
          "author": "Arun Alejandro Varma",
          "authorURL": "https://www.linkedin.com/in/arunalejandro/",
          "affiliations": [
            {
              "name": "LGO, MIT",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/staging/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Foley-to-video generating video from environmental audio</h1> <p>In this blog we will explore the optimal architecture for generating video from environmental sound inputs.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#background">Background</a></div> <div><a href="#objective">Objective</a></div> <div><a href="#plan">Plan</a></div> <div><a href="#bibliography">Bibliography</a></div> </nav> </d-contents> <h2 id="background">Background</h2> <p>In filmmaking, a “Foley” is the “reproduction of everyday sound effects that are added to films in post-production to enhance audio quality.” Conversely, we aim to produce what we have dubbed (no pun intended) “Antifoleys” – the reproduction of video that could feasibly have accompanied the inputted audio. Below we discuss our plan of action, along with conceptual and technical questions we will explore.</p> <h2 id="objective">Objective</h2> <p>A lot of research has been done in music generation and audio detection, as well as text-to-video and video generative models. The goal of this project is to leverage existing data and models, and explore a novel application of these models when working together.</p> <h2 id="plan">Plan</h2> <p>The success of our project depends on accomplishing two things: identifying a successful architecture and gathering the right data. To achieve these, we ask ourselves guiding conceptual questions. We do not have answers to all of them yet – it is an ongoing discussion.</p> <ul> <li>What will be our inputs and outputs? We plan to separate video and audio channels from the same data, then use audio as inputs and video as outputs.</li> <li>What type of data preprocessing is necessary? We will use the harmonic representation of the input and compare it to the raw waveform of the audio input. This may yield more promising encodings for the audio.</li> <li>Does our model need a temporal sense / sequential embedding? On one hand, perhaps not – at minimum, we simply need a 1:1 mapping between each second of video and audio. On the other hand, if we want the video to seem coherent, our model probably does need a sense of sequentiality. This will help determine the architectures we select.</li> <li>Do we leverage an existing model and apply transfer learning? Do we build it from scratch?</li> <li>What architectures lend itself well to this task? Since we are associating two different forms of data, representation learning might be a strong candidate. We have considered a form of Autoencoder, where the encoder encodes the audio and the decoder decodes to video.</li> <li>Where do we find our data?</li> <li>Where do we find existing models?</li> </ul> <h2 id="bibliography">Bibliography</h2> <ol> <li>ACOUSTIC SCENE CLASSIFICATION: AN OVERVIEW OF DCASE 2017 CHALLENGE ENTRIES</li> <li>Data-Driven Harmonic Filters for Audio Representation Learning</li> <li>Conditional GAN with Discriminative Filter Generation for Text-to-Video Synthesis</li> </ol> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/staging/assets/bibliography/2023-11-08-foley-to-video.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>