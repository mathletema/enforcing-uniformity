<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Sample Blog Post | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="Your blog post's abstract. This is an example of a distill-style blog post and the main elements it supports."/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/blog/2022/alleviating-catastrophic-forgetting-in-classification-tasks-through-strategic-filter-dropout/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/staging/assets/js/distillpub/template.v2.js"></script> <script src="/staging/assets/js/distillpub/transforms.v2.js"></script> <script src="/staging/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Sample Blog Post",
      "description": "Your blog post's abstract. This is an example of a distill-style blog post and the main elements it supports.",
      "published": "December 1, 2022",
      "authors": [
        {
          "author": "Pieter Feenstra",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        },
        {
          "author": "Nicholas Dow",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/staging/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Sample Blog Post</h1> <p>Your blog post's abstract. This is an example of a distill-style blog post and the main elements it supports.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#motivation">Motivation</a></div> <div><a href="#intuition">Intuition</a></div> <div><a href="#implementation">Implementation</a></div> <div><a href="#evaluation">Evaluation</a></div> <div><a href="#references">References</a></div> </nav> </d-contents> <h2 id="motivation">Motivation</h2> <p>Catastrophic forgetting, also known as catastrophic inference, is a phenomenon in machine learning which refers to the tendency of models to entirely lose inference on a previous task when trained on a new task. This can be attributed to the idea that the weights learned for the previous task are significantly altered during the learning process for the new task. In effect, the model’s understanding of the previous task is overwritten. If a perfect classifier, trained for classifying a dog versus a cat is trained to be a perfect classifier for classifying between a car and a truck, the model will lose valuable insights into the former task, even if updates in weights are not relevant to the new task.</p> <h2 id="intuition">Intuition</h2> <p>To test generalizability of networks and the retention of training of different tasks, we will attempt to target specific neurons in a trained network to “keep” or “drop-out” and then continue to train the modified model on new tasks. By “dropping-out” we mean exclude them from training in the next tasks; the hope of this would be to choose a subset of neurons in the model to prevent further training on. After further training, we would check how much performance the model retained on the original task. We could extend this further to do the same “drop-out” across several tasks and then compare a model produced by “drop-out” to that of a model just trained on the whole dataset. In terms of “drop-out” neuron choices, the most obvious choice would be the neurons most active in the classification task just trained with the idea that the most active neurons have the highest “contribution” to a correct classification. Another choice would be to choose neurons with the highest discriminative ability between the classes in the task, so the neurons that have the highest change in average value when classifying different samples. Within this general idea, there are a variety of avenues to explore: how many k neurons should be “dropped-out” or preserved from each training task? How does restricting the “drop-out” to only certain depths of the network affect performance?</p> <h2 id="implementation">Implementation</h2> <p>We will assess the proposed idea with an image classification task. We will make use of publicly available datasets from Kaggle, including datasets for the prediction of cats versus dogs, cars versus bikes, lions versus cheetahs, and children versus adults. For prediction, we will use a convolutional neural network with cross entropy loss. Convolutional neural networks are suited for image-related tasks and will allow for relatively easy computation of the most activated neurons, which we will consider to be filters with the highest magnitude output. After training a model, we will freeze gradient descent on the k filters, choosing the filters by different selection metrics, and train on new data. K will be a hyperparameter that will be adjusted to optimize performance.</p> <h2 id="evaluation">Evaluation</h2> <p>We will evaluate this model through a comparative analysis with a baseline model. We will train both models on an initial dataset, freeze k parameters of our model, then retrain the models on a second dataset. We will then compare the accuracy on some test set of the initial data. We will repeat this with varying values of k. Ultimately, we will compare our model with a model trained on all data at once.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/staging/assets/bibliography/2022-12-01-distill-example.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>