<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Understanding Limitations of Vision-Language Models | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="Staging website for the 2023 ICLR Blogposts track "/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/blog/2022/Vision_Language_Limitations/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/staging/assets/js/distillpub/template.v2.js"></script> <script src="/staging/assets/js/distillpub/transforms.v2.js"></script> <script src="/staging/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Understanding Limitations of Vision-Language Models",
      "description": "",
      "published": "December 1, 2022",
      "authors": [
        {
          "author": "Shelley Choi",
          "authorURL": "",
          "affiliations": [
            {
              "name": "MIT",
              "url": ""
            }
          ]
        },
        {
          "author": "Siddharth Somasundaram",
          "authorURL": "",
          "affiliations": [
            {
              "name": "MIT",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/staging/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Understanding Limitations of Vision-Language Models</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#initial-prompt">Initial Prompt</a></div> <div><a href="#proposal-overview">Proposal Overview</a></div> <div><a href="#potential-research-questions">Potential Research Questions</a></div> <ul> <li><a href="#bias-to-text-labels">Bias to Text Labels</a></li> <li><a href="#transfer-learning">Transfer Learning</a></li> </ul><div><a href="#references">References</a></div> </nav> </d-contents> <h2 id="initial-prompt">Initial Prompt</h2> <p>Joint vision/language models such as CLIP try to align vision and language latent spaces. This provides an extra level of visibility into the representations: for example, for a given image of a cat, its similarity to the text embedding of “a photo of a cat” typically captures how “cat-like” the image is. This project would involve studying the representation space of such models with respect to sensitive attributes/biases. For example, given photos of either men or women, which image embeddings are closer to the caption “a photo of a firefighter.” This project would involve performing a systematic study to identify biases in the representations of such models.</p> <h2 id="proposal-overview">Proposal Overview</h2> <p>The idea behind the project is to explore joint vision/language models that try to align vision and language latent spaces. In that search, we take a closer look at OpenAI’s Contrastive Language-Image Pre-training (CLIP) [1] released in Feb 2021 and Wayve’s GAIA-1 [2] introduced in June 2023. CLIP consists of a convolutional neural network that transforms an image, and a transformer neural network that transforms text. These networks use contrastive modeling to compare similarity between the image and text space, and its zero-shot learning capabilities allow generalization across a variety of new concepts. GAIA can generate videos of driving simulations from a variety of inputs such as video, text, and/or action inputs. These inputs are then encoded into a common representation of tokens that are fed into a transformer (world model) that predicts the next image tokens.</p> <p>Regarding this topic, we had several ideas for research questions. Based on instructor feedback, we’re hoping to focus on one of them for the final project.</p> <h2 id="potential-research-questions">Potential Research Questions</h2> <h3 id="idea-1-investigating-and-mitigating-bias-to-text-labels">Idea #1: Investigating and Mitigating Bias to Text Labels</h3> <p>The first idea we were thinking of is related to contrastive learning with augmentations in label space instead of input space. The goal of contrastive learning is to ensure a constant output with respect to certain variations in the input. We note that vision-language models (e.g. GAIA, CLIP) are trained with text labels for the image inputs. However, a single text description is not a unique identifier of an image; there are many possible descriptions of a single image. For example, the text label of an image might take the form “Dad sitting on the couch”. An equally valid, but different, text label would be “A person napping on the sofa”. How would vision-language models handle these different cases?</p> <p><em>Scientific Question: Can augmentations in label space allow GAIA, CLIP, etc. to learn better representations with fewer data points?</em></p> <ul> <li>Will the text encoder map each of these two texts to similar latent spaces?</li> <li>How would downstream task performance be affected by using multiple label augmentations?</li> <li>If performance improves, could label augmentations enable training and convergence with fewer data samples?</li> </ul> <p><em>Possible Outcomes</em></p> <ul> <li>Either these models learn representations that map multiple labels to similar points in feature space, or</li> <li>the choice of text label affects how features in image space are encoded</li> </ul> <h3 id="idea-2-addressing-limitations-via-transfer-learning">Idea 2: Addressing Limitations via Transfer Learning</h3> <p>We also wanted to ask: How can multi-modal generative AI models trained on a specific dataset be generalized and decrease bias? GAIA, in particular, was specifically trained using Wayve’s UK urban driving data. In the UK, drivers drive on the left hand side of the road. Furthermore, the dataset primarily focuses on urban roads, where there are clearly defined lines that indicate asphalt concrete roads. We want to see if this model can also be applied to countries that don’t necessarily follow these “rules” that the GAIA model learned. Can the model also discover other “rules” where vehicles drive on the right side of the road in other parts of the world, or where roads do not have clear structure in less developed countries?</p> <p>GAIA unfortunately does not publish its data, so we cannot know whether the model truly achieves data symmetry. However, we could take the following approaches in transfer learning, where we can likely reuse the GAIA model and generalize to other places with different rules. Alternative options or further details will likely come as we learn more about transfer learning in class during Week 11.</p> <p><em>Approach 1: Dual-encoder contrastive learning</em></p> <p>Dual-encoder contrastive learning, which is part of the contrastive learning that maximizes the similarity between similar items and minimizes the similarity between dissimilar items, allows consideration of two different data domains. We define dual-encoder contrastive loss to be the following, where the two data domains \(\chi_1\) and \(\chi_2\) represent images and text, respectively. The encoder \(f_1\) can map images to a fixed-dimensional space using convolutional neural networks (CNN), and the encoder \(f_2\) can map text using a transformer:</p> <p>After training, a decoder can take in the image and text embeddings to generate a series of images \(V_i\) that constitute a video \(V\). Once we learn the meaningful representations of the multimodal input data that can be mapped onto a singular space, it becomes easier to understand their relationship to aid in domain adaptation—we can utilize a similar multi-modal structure.</p> <p><em>Approach 2: Few-shot learning</em></p> <p>Few-shot learning helps the model to recognize and evaluate situations where there may be sparse data. It would address GAIA’s lack of diverse data. For example, it would allow GAIA to be expanded to images from other countries (that may have more side roads or undefined roads) to text that describes situations that are rarely encountered in the UK (extreme weather situations such as a tornado) without having extensive labeled data. Once we are able to capture the relationships between the different domains, where we can identify potential “base classes,” we can use that information for few-shot learning and achieve good generalization for GAIA. Some techniques might involve recurrent neural networks (RNN) or siamese networks.</p> <h2 id="references">References</h2> <ol> <li>Radford et al., <em>“Learning transferable visual models from natural language supervision”</em>, ICML 2021</li> <li>Hu et al., <em>“GAIA-1: A Generative World Model for Autonomous Driving”</em>, arXiv 2023</li> </ol> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/staging/assets/bibliography/2022-12-01-distill-example.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>