<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://deep-learning-mit.github.io/staging/feed.xml" rel="self" type="application/atom+xml"/><link href="https://deep-learning-mit.github.io/staging/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-13T04:44:54+00:00</updated><id>https://deep-learning-mit.github.io/staging/feed.xml</id><title type="html">6.S898 Deep Learning Blogs 2023</title><subtitle>Staging website for the 2023 ICLR Blogposts track </subtitle><entry><title type="html"></title><link href="https://deep-learning-mit.github.io/staging/blog/2023/2023-11-09-convex_optimization_layer_deep_learning/" rel="alternate" type="text/html" title=""/><published>2023-12-13T04:44:54+00:00</published><updated>2023-12-13T04:44:54+00:00</updated><id>https://deep-learning-mit.github.io/staging/blog/2023/2023-11-09-convex_optimization_layer_deep_learning</id><content type="html" xml:base="https://deep-learning-mit.github.io/staging/blog/2023/2023-11-09-convex_optimization_layer_deep_learning/"><![CDATA[<h2 id="convex-optimization-as-a-layer-in-neural-network-architectures">Convex Optimization as a Layer in Neural Network Architectures</h2> <p>Convex optimization is a well-studied area of operations research. There has recently been an insurgence of work relating the field to machine learning. Agrawal et al. <d-cite key="agrawal2019differentiable"></d-cite> propose a method known as ``disciplined parameterized programming’’, which maps the parameters of a given convex program to its solution in a differentiable manner. This allows us to view instances of convex optimization programs as functions mapping problem-specific data (i.e input) to an optimal solution (i.e output). For this reason, we can interpret a convex program as a differentiable layer with no trainable parameters in the same way as we think of ReLU as a layer in a deep neural network. Past work (<d-cite key="amos2021optnet"></d-cite>, <d-cite key="barratt2019differentiability"></d-cite>) has primarily focused on providing methods for differentiability of the convex optimization layer. However, an unexplored question remains: for which types of machine learning problems does this architecture provide an edge over other architectures?</p> <h2 id="the-role-of-convex-optimization-layers-for-various-machine-learning-tasks">The Role of Convex Optimization Layers for Various Machine Learning Tasks</h2> <p>We hypothesize that architectures which leverage convex optimization layers may perform better on some machine learning tasks than others. CNNs have become the gold standard for solving supervised learning prediction tasks from image data. Transformers are now the go-to architecture in generative modeling when working with language. However, it remains unclear in which settings, if any, we may rely on convex optimization layers as the default choice of architecture.</p> <p>This project explores when such an architecture might be well-suited in machine learning. Specifically, we will implement a disciplined parametrized program for three separate tasks, in very different types of machine learning problems. We will then compare the performance of convex optimization as a layer between these tasks, using various metrics and baselines. This will provide insight as to which machine learning tasks are best suited for architectures with convex optimization layers.</p> <h3 id="supervised-learning">Supervised Learning</h3> <p>We consider the supervised learning problem of predicting the solution to a sudoku puzzle from its image representation. We will compare against baseline CNN or MLP models, and we will compare the accuracy and amount of training needed across these architectures. We will render solutions to sudoku puzzles in the context of convex optimization, and we hypothesize that the inductive bias of our architecture will provide better performance from existing architectures. In particular, we hypothesize that convex optimization as a layer will require less training and higher accuracy than for MLP and CNN architectures.</p> <h3 id="reinforcement-learning">Reinforcement Learning</h3> <p>We consider the control problem of steering a car above a hill, otherwise known as MountainCar, from the OpenAI gym benchmark of RL environments. We can model the problem with quadratic reward, and linear transition function, so that the optimal controller would be quadratic in state. By contextualizing the action as a solution to a convex optimization problem, we can enforce safety constraints explicitly, for stability of training of the agent. We will compare this model against baseline RL algorithms such as PPO, and will compare standard RL metrics, such as mean reward.</p> <h3 id="generative-modeling">Generative Modeling</h3> <p>We consider the generative learning problem of sampling maps for atari video games which satisfy specific conditions, such as the location of blocks or coins. We can make the data samples be solutions to an optimization problem, which enforces certain constraints on the generated solution, such as the locations or colors of features in the game. Then, by adding noise, and predicting the mean of noisy samples, we can generate fresh valid configurations also satisfying our optimization constraints. We will test the accuracy of our architecture by testing its accuracy across various tests and environments.</p>]]></content><author><name></name></author></entry><entry><title type="html">Imposing uniformity through Poisson flow models</title><link href="https://deep-learning-mit.github.io/staging/blog/2023/enforcing-uniformity/" rel="alternate" type="text/html" title="Imposing uniformity through Poisson flow models"/><published>2023-12-12T00:00:00+00:00</published><updated>2023-12-12T00:00:00+00:00</updated><id>https://deep-learning-mit.github.io/staging/blog/2023/enforcing-uniformity</id><content type="html" xml:base="https://deep-learning-mit.github.io/staging/blog/2023/enforcing-uniformity/"><![CDATA[ <p>Most objects encountered in machine learning are extremely high dimensional. For example, a relatively small $512$x$512$ RGB image has over $750,000$ dimensions. However most of this space is empty, that is the set of well-formed images form an extremely small subset of this large space.</p> <p>Thus a useful task in machine learning is to map this large space into a much smaller space, such that the images we care about form a compact organized distribution in this new space. This is called representation learning. For such a map to be useful, there are two key features. Firstly the representations should be useful for downstream tasks and not worse than the original representation. Thus they should preserve as much of the useful data as possible. Secondly, they should be relatively task agnostic and help across a diverse array of such downstream tasks. For example, word embeddings (such as those produced by BERT <d-cite key="bert"></d-cite>) can be used for a wide array of language tasks such as language modeling and generation to sentiment analysis. An important question is how to generally find such useful representations.</p> <p>Several methods exist. For example, autoencoders <d-cite key="autoencoder"></d-cite> attempt to learn maps that are essentially bijective over the dataset we care about. These ensure that important information is not lost during the transformation. Contrastive encoders attempt to learn maps that enforce similarity between representations of similar images. Contrastive encoders are seen to perform quite well on unsupervised representation learning tasks, and we will explore these in a bit more detail soon. Lastly, we can layers of already trained neural networks can be used as features as well. For example, layers of VGG-19 trained on ImageNet are useful features that contain much information about the style and content of the images <d-cite key="nst"></d-cite>.</p> <p>It is important to further quantify what makes a useful representation from a theoretical standpoint. Wang and Isola <d-cite key="isola-alignment"></d-cite> introduced concepts of alignment and uniformity to explain why contrastive encoders perform well. Alignment is the idea that similar objects should have close feature vectors in the representation space. Uniformity is the idea that the set of well-formed objects should cover the representation space uniformly.</p> <p>In this post, we will further examine how uniformity affects the quality of representations. To do this, we will use Poisson flows. As we shall see, Poisson flows are an incredibly useful tool to enforce uniformity. We show that enforcing uniformity on well-aligned features can improve representations as measured by their performance on downstream tasks.</p> <h2 id="notation">Notation</h2> <p>We introduce several notations to make talking about representations easier. Let $\mathcal{X}$ be our original space of the data, and let $p_{\mathrm{x}}$ be the distribution of the data. Let $\mathcal{Y}$ be any representation space, and let $f: \mathcal{X} \to \mathcal{Y}$ be a mapping from the original space to the representation space. If $\mathrm{y} = f(\mathrm{x}), \ \mathrm{x} \sim p_{\mathrm{x}}$, then let $\mathrm{y} \sim p_{f}$ and where $p_{f}$ is the new distribution after $f$.</p> <p>We will also have a notion of similarity. Let $p_{\mathrm{pos}}(x_1, x_2)$ be a joint probability distribution that quantifies this similarity. We assume that $p_{\mathrm{pos}}$ satisfies</p> \[\begin{aligned} p_{\mathrm{pos}}(x_1, x_2) &amp;= p_{\mathrm{pos}}(x_2, x_1) \\ \int_{x_2} p_{\mathrm{pos}}(x_1, x_2) d x_2 &amp;= p_{\mathrm{x}}(x_1) \end{aligned}\] <h2 id="alignment-and-uniformity">Alignment and Uniformity</h2> <p>As mentioned earlier, contrastive autoencoders learn useful representations by minimizing a distance metric for similar pairs, while maximizing the same for dissimilar pairs <d-cite key="isola-cmc">. Thus if $D(x_1, x_2)$ is some distance metric of $\mathcal{Y}$, contrastive encoders maximize $d(x, x^+)$ for positive pairs, while minimizing $d(x, x^-)$ for negative pairs.</d-cite></p> <p>In their most common formulation, they set $\mathcal{Y}$ as the hypersphere $\mathcal{S}^d \subset \mathbb{R}^d$, and use cosine similarity <d-cite key="SimCLR"></d-cite> $d(x_1, x_2) = x_1^T x_2$ as the similarity metric. Then the loss function becomes <d-footnote>$M$ and $\tau$ are hyper parameters</d-footnote></p> \[\mathcal{L} \triangleq \mathbb{E}_{(x, x^+) \sim p_{\mathrm{pos}}, \{x_i^-\}_{i=1}^M \overset{\mathrm{iid}}{\sim} p_{\mathrm{x}}} \left[ \frac {e^{f(x)^T f(x^+)^T / \tau}} {e^{f(x)^T f(x^+)^T / \tau} + \sum_{i=1}^{M} e^{f(x)^T f(x_i^-)^T / \tau}} \right]\] <p>These encoders have been successful at several image representation tasks. Wang and Isola explained their performance through alignment and uniformity. Alignment, is simply the the quality that similar images are close together in the representation space. This is clearly present in contrastive encoders, as one of their goals is indeed to minimize</p> \[\mathcal{L}_{\mathrm{alignment}} \triangleq \mathbb{E}_{(x, x^+)\sim p_{\mathrm{pos}}} \left[ D(x, x^+) \right]\] <p>However, Wang and Isola also stated that uniformity was an equally important feature of contrastive architectures. That is, when training the contrastive loss to learn an encoder $f$, the new probability distribution $p_{f}$ is close to uniform. They showed that using $L_2$ norm as a distance metric and using Gaussian kernels to promote uniformity, learned representations perform better than those learned by contrastive learning.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-enforcing-uniformity/alignment_uniformity-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-enforcing-uniformity/alignment_uniformity-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-enforcing-uniformity/alignment_uniformity-1400.webp"/> <img src="/staging/assets/img/2023-11-09-enforcing-uniformity/alignment_uniformity.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1: Alignment and Uniformity. In figure (a), we see the quality of alignment, i.e. similar images are close to each other in the representation space. In figure (b), we see the quality of uniformity, i.e. images form a uniform distribution across the representation space. Image borrowed from <d-cite key="isola-alignment"></d-cite> </div> <p>Why does uniformity help? Firstly, it acts as a regularization term. This is because if we tried to learn representations that maximized alignment without any target for uniformity, then a map that just takes all input vectors to zero would trivially minimize the loss. Yet this would be an extremely bad representation. However, aside from regularization, uniform distributions also have maximal self-entropy. Thus their importance can be explained equally well through some sort of minimizing loss of information. Indeed this is how <d-cite key="isola-alignment"></d-cite> explains it.</p> <p>In this post we will investigate this even further. In particular, if regularization is the only effect that uniformity has on representations, then slightly nudging already aligned representations to make them uniform should not improve their quality. This is exactly what we will do, and we will do this through Poisson Flows.</p> <h2 id="poisson-flows">Poisson Flows</h2> <p>If you let a planar positive distribution of charges slightly above $z=0$ loose, then they will repel each other. If you stop them at some large enough distance $R$ from the origin, then their distribution approaches uniform as $R \to \infty$. This is very interesting, and what’s even more interesting is that this fact generalizes to arbitrary dimensions. Thus such fields allow a convenient way to map arbitrary high-dimensional distributions to uniform distributions. Poisson flow generative models proposed by Xu and Liu <d-cite key="poisson-flow"></d-cite> exploit this property for image generation; by sampling uniformly from the hemisphere, one can iterate through the backward ODE and thus sample from $p_{\mathrm{x}}$. We shall use it to impose uniformity on well-aligned features.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-enforcing-uniformity/poisson-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-enforcing-uniformity/poisson-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-enforcing-uniformity/poisson-1400.webp"/> <img src="/staging/assets/img/2023-11-09-enforcing-uniformity/poisson.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2: Evolution of data points through a Poisson field. As we can see, arbitrary distributions are mapped to uniform. Further the mapping is continuous. Borrowed from <d-cite key="poisson-flow"></d-cite> </div> <p>Say we have a probability distribution $p_{\mathrm{y}}$ over $\mathcal{Y}_1 = \mathbb{R^d}$. Set this distribution at the $z = 0$ plane <d-footnote>here z-dimension refers to the new dimension we recently augmented the dataset with</d-footnote> in the expanded space $\tilde{\mathcal{Y}}_1 = \mathcal{Y} \times \mathbb{R}$. Let the electric field at an arbitrary point in $\tilde{\mathcal{Y}}_1$ be defined as</p> \[E_{p_{\tilde{\mathrm{y}}}}(\tilde{y}) = \int_{\tilde{y}'} \frac{\tilde{y} - \tilde{y'}}{\|\tilde{y} - \tilde{y'} \|_2^{d+1}} \cdot p_{\tilde{\mathrm{y}}}(\tilde{y}') d\tilde{y}'\] <p>Let $\mathrm{y} \sim p_{\mathrm{y}}$. Evolve $\tilde{\mathrm{y}} = (\mathrm{y}, 0) \in \tilde{\mathcal{Y}_1}$ according to the ODE</p> \[\frac{d\tilde{\mathrm{y}}}{dt} = E_{p_{\tilde{\mathrm{y}}}}(\tilde{y})\] <p>Let the final point be $f_{\mathrm{poisson}}(\mathrm{y}; p_{\mathrm{y}})$. Then the distribution of $p_{f_{\mathrm{poisson}}}(\cdot)$ approaches uniform as $R \to \infty$.</p> <p>In practice, since we want to take $s = 0$ to $R$, we do a change of variables to write the ODE as</p> \[\frac{d \tilde{\mathrm{y}}}{ds} = \frac{1}{E_{p_{\tilde{\mathrm{y}}}}(\tilde{\mathrm{y}})^T \tilde{\mathrm{y}}} \cdot E_{p_{\tilde{\mathrm{y}}}}(\tilde{\mathrm{y}})\] <p>Note that the field stated here isn’t actually used directly, it is rather learned through a deep neural network. This is possible since the integral can be replaced with an expectation, which itself can be approximated through Monte-Carlo methods.</p> <p>Since Poisson flows allow us to map arbitrary distributions to uniform ones, while preserving continuity; they are an extremely powerful tool to further understand the effects of uniformity. This brings us to our main hypothesis</p> <h2 id="hypothesis">Hypothesis</h2> <blockquote> <p>Assume that uniformity acts more than just a regularizing term for learning useful representations. Then if we take any well-aligned features that have good downstream performance, and apply a continuous map that imposes uniformity, our new features should perform better at downstream tasks</p> </blockquote> <p>This is because if uniformity is simply a regularizing term, then training them for the downstream task is the best we can do. This hypothesis itself is counterintuitive because the original features should already be well-trained against the task at hand. However, surprisingly, this hypothesis seems to hold true. To show this, we describe the following experiment.</p> <h2 id="experiment">Experiment</h2> <p>We consider the pen-ultimate layer of AlexNet <d-cite key="alexnet"></d-cite> trained on CIFAR-10 <d-cite key="cifar-10"></d-cite> as our initial features. These features must be well aligned, as linear decision boundaries are able to accurately classify them into their classes.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-enforcing-uniformity/flow-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-enforcing-uniformity/flow-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-enforcing-uniformity/flow-1400.webp"/> <img src="/staging/assets/img/2023-11-09-enforcing-uniformity/flow.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3: A representation of how the features should evolve. (a) Initial representation is seemingly random and hard to classify with. (b) After training a classifier, the penultimate layer is well clustered between the various features. (c) Features after learning a Poisson field, and sending the data through it. <d-footnote>Note that images (a) and (b) are for representation purposes only. However image (c) is indeed produced through a learned Poisson field from data points in (b)</d-footnote> </div> <p>We take these features and learn a corresponding Poisson field. For our predicted poisson field, we use a relatively small fixed-size two-hidden layer network.</p> <p>We finally pass our features through this Poisson field and train a linear classifier on top of the final learned representations. We compare this accuracy against the original accuracy.</p> <p>A summary of our approach is given in the figure below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-enforcing-uniformity/architecture-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-enforcing-uniformity/architecture-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-enforcing-uniformity/architecture-1400.webp"/> <img src="/staging/assets/img/2023-11-09-enforcing-uniformity/architecture.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4: Overview of architecture </div> <p>Further training details are given in <a href="#appendix-a-training-details">Appendix A</a>.</p> <h2 id="results">Results</h2> <p>The results are given in the below table.</p> <table> <thead> <tr> <th>Architecture</th> <th>Train accuracy</th> <th>Test accuracy</th> </tr> </thead> <tbody> <tr> <td>AlexNet</td> <td>88%</td> <td>82%</td> </tr> <tr> <td>AlexNet + Poisson Flow <em>(ours)</em></td> <td>95%</td> <td>85%</td> </tr> </tbody> </table> <p>Here we see that our method outperforms a well-trained AlexNet considerably.</p> <h2 id="conclusion">Conclusion</h2> <p>This is a surprisingly nice improvement. Note that the Poisson flow post-processing step is completely unsupervised. This seems to hint that having a uniform prior is helpful for reasons other than just regularization.</p> <p>It would be extremely interesting to develop an entirely unsupervised architecture based on Poisson flow. This would begin by using an unsupervised method to learn well-aligned features. A suitable loss candidate could possibly be just a contrastive loss, with L2 norm as a distance metric:</p> \[\mathcal{L} \triangleq \mathbb{E}_{(x, x^+) \sim p_{\mathrm{pos}}, \{x_i^-\}_{i=1}^M \overset{\mathrm{iid}}{\sim} p_{\mathrm{x}}} \left[ \|x - x^+\|_2^{\alpha} - \lambda \sum_{i=1}^{M} \|x - x_i^{-}\|_2^{\beta} \right]\] <p>Then passing these well-aligned features through a Poisson flow would enforce uniformity. Such a proposed architecture could be worth exploring.</p> <hr/> <h2 id="appendices">Appendices</h2> <p>See <a href="https://github.com/mathletema/poisson-representations">https://github.com/mathletema/poisson-representations</a> for code.</p> <h3 id="appendix-a-training-details">Appendix A: Training details</h3> <p>We used a version of AlexNet similar to that given in Isola’s paper, such that the pen-ultimate layer was 128 neurons wide. We trained this network against cross entropy loss for 20 epochs using Adam as an optimizer.</p> <p>After this, we moved the features from $\mathbb{R}^{128}$ to $\mathbb{R}^{129}$ by setting $z = 0$. We then learned a Poisson field for this network similar to <d-cite key="poisson-flow"></d-cite>. We use the default values of $\tau, \gamma, \sigma$ as the original paper, but used $M = 20$ as a consequence of our reduced dimension size. We trained this Poisson field with a large batch size of $1024$ and a small batch size of $128$. We trained this over $200$ epochs.</p> <p>We then passed the features through the Poisson field. To simulate the ODE, we used Euler’s method with a small delta of $0.01$ and $100$ steps. Using RK4 might produce better results, and we leave this to future work.</p> <p>We finally trained a logistic classifier on top of these final representations, and printed train and test accuracies.</p>]]></content><author><name>Ishank Agrawal</name></author><summary type="html"><![CDATA[Uniformity and alignment are used to explain the success of contrastive encoders. Can we use already trained, well-aligned features and impose uniformity to increase their quality and performance on downstream classification tasks?]]></summary></entry><entry><title type="html">Rep Learning For Rec Systems</title><link href="https://deep-learning-mit.github.io/staging/blog/2023/rep-learning-for-rec-systems/" rel="alternate" type="text/html" title="Rep Learning For Rec Systems"/><published>2023-12-01T00:00:00+00:00</published><updated>2023-12-01T00:00:00+00:00</updated><id>https://deep-learning-mit.github.io/staging/blog/2023/rep-learning-for-rec-systems</id><content type="html" xml:base="https://deep-learning-mit.github.io/staging/blog/2023/rep-learning-for-rec-systems/"><![CDATA[<h1 id="final-project-proposal">Final Project Proposal</h1> <p>Much of our social media and content platforms such as TikTok, Instagram, YouTube, Netflix, etc. utilize recommender systems for provided personalized content and feeds. A crucial factor in delivering good recomendation system having an expressive embedding of user data and history.</p> <p><strong>I want to explore different representation learning algorithms to generate different embeddings to be used for recommendations / clusters and evaluate their accuracy.</strong></p> <p>The application I will use to test this on will depend on the data availability. One domain I was initially thinking of was movie recommendation due to the prevalence of open-source data.</p> <p>Some ideas that I have read about:</p> <ul> <li>Use large language models directly as a resource for recommending user content based on past likes and history</li> <li>Use graph / transformer neural networks by modeling user history as sequential data</li> <li>Use contrastive learning / autoencoders to build embedding vector of user past likes and history</li> </ul> <h3 id="potential-references">Potential References:</h3> <ul> <li><a href="https://arxiv.org/pdf/2310.18608.pdf">Embedding in Recommender Systems: A Survey</a></li> <li><a href="https://arxiv.org/pdf/2311.01343.pdf">Collaborative Large Language Model for Recommender Systems</a></li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Final Project Proposal]]></summary></entry><entry><title type="html">Graph Deep Learning for Articulated Objects - Project Proposal</title><link href="https://deep-learning-mit.github.io/staging/blog/2023/graph-articulated-objects/" rel="alternate" type="text/html" title="Graph Deep Learning for Articulated Objects - Project Proposal"/><published>2023-11-16T00:00:00+00:00</published><updated>2023-11-16T00:00:00+00:00</updated><id>https://deep-learning-mit.github.io/staging/blog/2023/graph-articulated-objects</id><content type="html" xml:base="https://deep-learning-mit.github.io/staging/blog/2023/graph-articulated-objects/"><![CDATA[<h2 id="project-vision">Project Vision</h2> <p>Recent advancements in generative AI have transformed robotic capabilities across all parts of the stack, whether in control, planning, or perception. As self-driving cars roll out to public roads and factory assembly-line robots become more and more generalizable, embodied intelligence is transforming the way that humans interact with each other and automate their daily tasks.</p> <p>Across the robotic manipulation stack, I am most interested in exploring the problem of perception; using the limited sensors available to it, how can a robot gain a rich understanding of its environment so that it can perform a wide array of general tasks with ease? Developments in inverse graphics, such as NeRF and recent models like PointE or DreamGaussian have allowed roboticists to harness the power of deep learning to make more detailed scene representations, enabling their robots to leverage 3D inputs to perform complicated tasks.</p> <p>One direction that I have been very interested in exploring is in developing robust representations that accurately represent a scene’s kinematic constraints as well, which will allow robots to make plans and predict the outcomes of their actions in an easier way.</p> <p>In this vein, I hope to explore the feasibility of incorporating graphical information to generate articulated URDF models that can be used in downstream robotics applications. Since articulated objects can be expressed as graphs, I want to specifically learn graph properties of an object either from a single image or a series of a few frames of a short video, with the goal of generating a URDF of the object at the very end.</p> <h2 id="related-work">Related Work</h2> <p>The first work to explore the use of graph denoising networks to generate URDF is NAP: Neural Articulation Prior<d-cite key="lei2023nap"></d-cite>, which conditions its generation on either the object’s structural graph or a representation of its partial geometry. Their work, while an important step in the direction of URDF generation, often generates physically implausible outputs that don’t actually represent the ground truth in the best way. Other works, such as URDFormer<d-cite key="chen2023urdformer"></d-cite>, use a transformer architecture to train on a large dataset of procedurally generated/annotated pairs of URDFs with corresponding images, training a model that can generate statistically accurate URDF models that roughly align with an image given to the model as input.</p> <p>NAP and URDFormer both generate realistic models that can be used as simulation assets, but struggle to generate an accurate model of real-world 3D data, which is core to closing the Real2Sim gap. Closest to my goal is Ditto<d-cite key="jiang2022ditto"></d-cite>, which learns an implicit neural-representation for a point cloud before and after being moved, constructing the URDF representation using a learned correspondence between frames. Ditto’s approach using multiple frames to make its reconstruction is critical, because articulation models are inherently ambiguous without information about joint constraints.</p> <p>However, their main drawback is their assumption of segmenting a point cloud into only two parts. More complicated objects, such as cupboards with handles or multiple drawers, are not supported by their method, which leaves room to explore methods that can infer the whole kinematic tree. To this end, I hope to explore graph-based approaches that are more easily able to extend a method like Ditto to more complicated objects.</p> <p>This project would be successful if I am able to explore the use of novel loss function/algorithmic innovation to perform better than NAP or Ditto at real-world scenarios, perhaps one that can also be conditioned on text-based prompting or using priors from VLMs like GPT4-Vision.</p> <h2 id="outline-of-steps">Outline of Steps</h2> <ol> <li>Collect a dataset of labeled URDF assets with known natural language prompts along with URDF and geometric information.</li> <li>Reproduce Ditto’s work and fully understand how it is working, trying it on various cases to get a sense of where the paper’s method breaks.</li> <li>Reproduce NAP’s work and figure out how it encodes and learns kinematic structure.</li> <li>Make adjustments to Ditto’s framework of URDF generation. This will likely involve slightly modifying Ditto’s architecture to support graph-based intermediate representations instead of solely working in the realm of unstructured point clouds. Another approach may be to incorporate GPT4-Vision or other pre-trained image-based priors to segment images into prospective rigid bodies. Depending on the results, this project may provide valuable insights into the pros and cons of either approach when extending Ditto to a general multi-link setting.</li> </ol>]]></content><author><name>Anirudh Valiveru</name></author><summary type="html"><![CDATA[In the fields of robotics and computer graphics, learning how to generate articulated objects that look and function accurately to the real world. The conditional generation of CAD/URDF models will be a significant advantage in the field of Real2Sim and is a crucial step to enabling generalizable robotics in the real world. Recent advancements in generative models, including diffusion, have opened up the possibilities of work in the supervised generation of data, ranging from images to molecular and even robot action information. This project explores the feasibility of the conditional generation of URDF data conditioned on a text prompt, leveraging graph neural networks to encode spatial/kinematic constraints.]]></summary></entry><entry><title type="html">Exploring limited and noisy datasets augmentation using denoising VAEs</title><link href="https://deep-learning-mit.github.io/staging/blog/2023/denoisingVAE/" rel="alternate" type="text/html" title="Exploring limited and noisy datasets augmentation using denoising VAEs"/><published>2023-11-11T00:00:00+00:00</published><updated>2023-11-11T00:00:00+00:00</updated><id>https://deep-learning-mit.github.io/staging/blog/2023/denoisingVAE</id><content type="html" xml:base="https://deep-learning-mit.github.io/staging/blog/2023/denoisingVAE/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Denoising autoencoders (DAE) are trained to reconstruct their clean inputs with noise injected at the input level, while variational autoencoders (VAE) are trained with noise injected in their stochastic hidden layer, with a regularizer (KL divergence) that encourages this noise injection. Denoising Variational Autoencoders (DVAEs) are an extension of the traditional variational autoencoder (VAE). The research paper “Denoising Criterion for Variational Auto-Encoding Framework” <d-cite key="denoisingVAE"></d-cite> elucidates how incorporating a denoising criterion into the VAE framework can significantly improve the robustness of the learned representations, thereby enhancing the model’s generalization ability over various tasks.</p> <h2 id="objective">Objective</h2> <p>The aim is -</p> <ol> <li>to develop a DVAE OR use a pre-trained model that is capable of extracting robust features from small and noisy datasets, such as the RETINA dataset for diabetic retinopathy diagnosis.</li> <li>test if generated synthetic data can supplement the original dataset, enhancing the performance in downstream tasks with scarce data/imbalanced classes.</li> </ol> <h2 id="research-questions-to-explore">Research questions to explore</h2> <ol> <li> <p><strong>Learning Robust representation and Generating Synthetic data using DVAEs:</strong> Can DVAEs dual capability of denoising input data and learning a generative model of the data distribution simultaneously be exploited to effectively learn robust representations from limited and noisy datasets and utilized to generate additional synthetic data (augmented dataset)?</p> </li> <li> <p><strong>Performance Enhancement for downstream tasks:</strong> How does the DVAE-generated synthetic data impact the performance metrics of downstream tasks, for example, severity classification?</p> </li> <li> <p><strong>Comaprison with traditional VAEs:</strong> How the learned representaion using DVAEs compare to traditional VAEs on the noisy data? Does the denoising aspect of DVAEs provide a tangible benefit over traditional VAEs in terms of improved accuracy? Is the DVAE-augmented data robust to variations in image quality, such as those caused by different imaging equipment in healthcare data?</p> </li> </ol> <hr/>]]></content><author><name>Pranay Agrawal</name></author><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Unraveling Social Reasoning in LLMs - A Decision Tree Framework for Error Categorization</title><link href="https://deep-learning-mit.github.io/staging/blog/2023/unraveling-social-reasoning-in-llms/" rel="alternate" type="text/html" title="Unraveling Social Reasoning in LLMs - A Decision Tree Framework for Error Categorization"/><published>2023-11-11T00:00:00+00:00</published><updated>2023-11-11T00:00:00+00:00</updated><id>https://deep-learning-mit.github.io/staging/blog/2023/unraveling-social-reasoning-in-llms</id><content type="html" xml:base="https://deep-learning-mit.github.io/staging/blog/2023/unraveling-social-reasoning-in-llms/"><![CDATA[<h1 id="unraveling-social-reasoning-in-llms-a-decision-tree-framework-for-error-categorization">Unraveling Social Reasoning in LLMs: A Decision Tree Framework for Error Categorization</h1> <h2 id="introduction">Introduction</h2> <p>Despite recent advances and the growth in scale of large language models (LLMs), it’s unclear how capable models are of reasoning, especially social commonsense reasoning. <d-cite key="huang_towards_2023"></d-cite> Tasks involving navigating complex social norms, emotions, and interactions remain a developing frontier in LLM research.</p> <p>Prior works like SOCIAL IQA <d-cite key="sap_socialiqa_2019"></d-cite>, ProtoQA, <d-cite key="boratko_protoqa_2020"></d-cite>, Understanding Social Reasoning in Language Models with Language Models <d-cite key="gandhi_understanding_2023"></d-cite> , and SOCIAL CHEMISTRY 101 have provided benchmarking datasets and techniques for social commonsense reasoning and social norms <d-cite key="forbes_social_2021"></d-cite>. Other works, such as Neural Theory-Of-Mind <d-cite key="sap_neural_2023"></d-cite>, explore why models struggle on these datasets and/or try to improve performance, such as by using knowledge graphs. <d-cite key="li_systematic_2022"></d-cite> <d-cite key="chang_incorporating_2020"></d-cite></p> <p>Therefore, our research has two goals: firstly, to expand upon previous research about the types of errors that LLMs make on social reasoning tasks, and secondly, to devise new categories that allow for better granularity when interpreting these mistakes that can help with finetuning models on these errors.</p> <h2 id="research-questions"><strong>Research Questions</strong></h2> <ul> <li>RQ1: What are underlying themes in social errors that large language models make?</li> <li>RQ2: Are there methods that could potentially address these errors?</li> </ul> <h2 id="methodology"><strong>Methodology</strong></h2> <h3 id="rq1"><strong>RQ1</strong></h3> <ol> <li> <p><strong>Preliminary Qualitative Analysis:</strong></p> <p>We will build upon benchmarking datasets based on Social IQA <d-cite key="sap_socialiqa_2019"></d-cite> and other datasets that provide categorization of social knowledge. An instance of such benchmark is the Social IQA Category dataset <d-cite key="wang_semantic_2021"></d-cite>, which considers four social knowledge categories: Feelings and Characteristics; Interaction; Daily Events; and Knowledge, Norm, and Rules. The authors of this paper found that RoBERTa-large performed the worst in the Feelings and Characteristics category, but did not provide general qualitative or quantitative observations about the types of errors made in each category. We want to better understand these sorts of errors made by the model in these domains.</p> <p>We plan to conduct an initial qualitative analysis to determine themes in common errors made in each of the four categories. For each model, we plan on sampling 20 or more questions in which the model does not answer correctly, then performing standard qualitative coding procedures to identify a set of common themes in errors for each category.</p> <p>Beyond testing the models listed in previous papers, we would like to explore how good GPT-4 is at answering these social commonsense reasoning questions. Given GPT-4’s improved capabilities compared to GPT-3, we suspect that this model will perform better; assessing its performance would allow other researchers to draw different insights into how architecture changes and expansions affect social reasoning.</p> </li> <li> <p><strong>Refinement and Analysis</strong></p> <p>Based on the insights gained from the preliminary qualitative analysis, we plan on devising more specific social knowledge categories than the four considered by the Social IQA Category dataset; we aim to construct categories based off of building a decision tree abstraction, where each dimension in the tree corresponds to a trait about the question.</p> <p>An example set of dimensions for the decision tree abstraction is as follows:</p> <ul> <li>Dimension 1: Social IQA Category’s four social knowledge categories</li> <li>Dimension 2: Type of question (effects, pre-conditions, stative descriptions, etc.) <ul> <li>Note: The authors in the original SocialIQA paper noted that BERT-large found questions related to “effects” to be the easiest and questions about “descriptions” to be the most challenging and claimed that models found stative questions difficult.</li> </ul> </li> <li>Dimension 3: Whether reasoning “is about the main agent of the situation versus others.” <ul> <li>In Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs, the authors argued that models perform much worse when the question is not about the main agent of the situation.</li> </ul> </li> </ul> <p>The goal here is to offer a more granular understanding of the categories of errors LLMs make on social reasoning questions. We will then perform another round of Preliminary Qualitative Analysis assessing themes in errors in each category to see if our categories improve on other papers’ categories.</p> </li> </ol> <p><strong>Experiment Setup</strong></p> <ul> <li>We will qualitatively assign each example under some combination of categories, trying to recreate the purported low performance on these social-reasoning datasets.</li> <li>Due to constraints in time, access, and computational resources, we plan on probing models like GPT-3 through through an API, similar to how it was done in the paper Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs. <ul> <li>Specifically, we will test on GPT-4 to see if these shortcomings still apply.</li> </ul> </li> </ul> <h3 id="rq2"><strong>RQ2</strong></h3> <ul> <li>What we do here is largely dependent upon RQ1 findings; strategies for addressing errors in social commonsense reasoning are largely contingent on the types and themes of errors identified in RQ1.</li> <li>We also consider existing approaches to enhancing capabilities of LLMs when it comes to social commonsense reasoning. Namely, in literature, many papers have experimented with the integration of external knowledge bases and fine-tuning models with semantic categorizations of social knowledge.</li> </ul> <h2 id="expected-outcomes">Expected Outcomes</h2> <ul> <li>A comprehensive catalog of error types and themes in LLMs concerning social reasoning.</li> <li>Insights into the comparative analysis of different models on social reasoning benchmarks.</li> </ul>]]></content><author><name>Nina Lei</name></author><summary type="html"><![CDATA[In this study, we investigate the challenge of social commonsense reasoning in large language models (LLMs), aiming to understand and categorize common errors LLMs make in social commonsense reasoning tasks. Our approach involves expanding upon the preliminary qualitative analyses of social reasoning errors, then developing a decision tree framework for more nuanced and fine-grained error categorization. We will test models such as GPT using this framework. We expect to better understand error types and themes in LLMs' social reasoning, offering insights for improving their performance in understanding complex social interactions.]]></summary></entry><entry><title type="html">A Transformer-Based Approach for Simulating Ecological Recovery</title><link href="https://deep-learning-mit.github.io/staging/blog/2023/A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/" rel="alternate" type="text/html" title="A Transformer-Based Approach for Simulating Ecological Recovery"/><published>2023-11-10T00:00:00+00:00</published><updated>2023-11-10T00:00:00+00:00</updated><id>https://deep-learning-mit.github.io/staging/blog/2023/A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery</id><content type="html" xml:base="https://deep-learning-mit.github.io/staging/blog/2023/A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>This project focuses on the application of Transformer models to conduct a spatial-temporal analysis of terrain and vegetation informed by satellite imagery and Digital Elevation Models (DEMs), with an added focus on geomorphological phenomena such as erosion and terrain incisions. The utilization of Transformer architecture aims to supersede the capabilities of traditional models by exploiting the ability to understand the complex relationship between spatial features and temporal evolution. The work will exploit the temporal resolution and spectral diversity of the datasets to not only reconstruct the ecological succession post-Mountaintop Removal (MTR) in Appalachia, but also to simulate the geomorphological processes that shape the terrain over time. By integrating dynamic elements, the project looks to provide predictive insights for environmental monitoring and landscape restoration, ensuring a deeper understanding of both the ecological and geomorphological features of landscape recovery.</p> <h3 id="objective">Objective</h3> <p>Employing Transformer models, a detailed analysis of Digital Elevation Models and satellite imagery is used to simulate the ecological recovery of terrains impacted by Mountaintop Removal. It utilizes the Transformer’s detailed analytical abilities, nown for its self-attention mechanisms, for precise land cover classification and to capture geomorphological changes, such as erosion and terrain incisions. These models excel in identifying patterns over time, critical for tracking the progression of natural regrowth and the effects of erosion. The combination of diverse datasets through the Transformer framework aims to generate an intricate and evolving 3D representation of the landscape, offering a clear depiction of its current state and potential recovery pathways, serving as an instrumental resource for informed environmental restoration and planning.</p> <h3 id="methodology">Methodology</h3> <p><u>Data Acquisition and Preprocessing</u></p> <p>The first stage will involve the collection of multi-spectral satellite imagery and high-resolution Digital Elevation Models (DEMs) of MTR-affected landscapes. This data will be preprocessed to ensure compatibility, which includes image normalization, augmentation, and the alignment of satellite imagery with corresponding DEMs to maintain spatial congruence. Preprocessing will also involve the segmentation of satellite data into labeled datasets for supervised learning, with categories representing different land cover types relevant to ecological states.</p> <p><u>Transformer Models for Spatial-Temporal Analysis</u></p> <p>Transformer models have exhibited remarkable success beyond their initial domain of natural language processing. Their unique self-attention mechanism enables them to capture long-range dependencies, making them a potentially good choice for complex spatial analysis. Vision Transformers, in particular, offer a new approach by treating image patches as tokens and allowing them to process the global context of an image effectively. This capability is beneficial for satellite imagery analysis, where understanding the broader environmental context is critical. Transformers designed for point cloud data, adapting to the inherent irregularities of LiDAR measurements, can potentially uncover intricate structural patterns and temporal changes within landscape data. With strategic approaches like transfer learning, transformers can overcome their computational resource complexity.</p> <p><u>Visualization and Simulation</u></p> <p>The final step will be the development of a 3D simulation environment using Unreal Engine. The simulation will visualize the predicted ecological states and changes over time, providing an interactive tool for users to explore the landscape recovery process. The interface will allow users to manipulate variables and observe potential outcomes of different restoration strategies in a virtual setting.</p> <h3 id="evaluation">Evaluation</h3> <p>For the spatial analysis of satellite imagery and LiDAR data, the evaluation will focus on the transformer’s ability to discern and classify diverse land cover types. The key metrics for this assessment will include accuracy, precision, recall, and the F1 score extracted from confusion matrices. The model should accurately identify and categorize ecological features from high-resolution imagery. Temporally, the performance will be evaluated based on its capacity to predict ecological changes over time. This involves analyzing the model’s output against a time series of known data points to calculate the Mean Squared Error (MSE) for continuous predictions or log-loss for discrete outcomes.</p>]]></content><author><name>Crystal Griggs</name></author><summary type="html"><![CDATA[This project employs Transformers for a comprehensive spatial-temporal analysis of post-Mountaintop Removal landscape recovery, utilizing satellite imagery and DEMs. It focuses on integrating geomorphological changes to predict ecological succession. Advanced Transformer architectures will be used to enhance the interpretability of complex spatial features over time, aiming to create an accurate 3D simulation environment for interactive exploration and effective restoration planning.]]></summary></entry><entry><title type="html">Predicting Social Ties Using Graph Neural Networks</title><link href="https://deep-learning-mit.github.io/staging/blog/2023/GNNs-&-Trust/" rel="alternate" type="text/html" title="Predicting Social Ties Using Graph Neural Networks"/><published>2023-11-10T00:00:00+00:00</published><updated>2023-11-10T00:00:00+00:00</updated><id>https://deep-learning-mit.github.io/staging/blog/2023/GNNs%20&amp;%20Trust</id><content type="html" xml:base="https://deep-learning-mit.github.io/staging/blog/2023/GNNs-&amp;-Trust/"><![CDATA[<h1 id="project-proposal-predicting-social-ties-using-graph-neural-networks">Project Proposal: Predicting Social Ties Using Graph Neural Networks</h1> <h2 id="abstract">Abstract</h2> <p>In the realm of social networks, the ability to predict social ties can provide invaluable insights into user behavior, community dynamics, and information diffusion. Graph Neural Networks (GNNs), with their capacity to learn from graph-structured data, offer a promising approach to this predictive task. This project proposes to explore the effectiveness of GNNs in predicting social ties and to examine whether these predictions can serve as a proxy for trust between individuals in a social network.</p> <h2 id="introduction">Introduction</h2> <p>With the proliferation of online social platforms, understanding and predicting social connections has become a topic of increased interest for both academic research and practical applications. Traditional machine learning methods often fall short in capturing the complex patterns within graph-structured data inherent to social networks. Graph Neural Networks, however, are uniquely suited for this purpose due to their ability to leverage node feature information and the topological structure of graphs.</p> <h2 id="objective">Objective</h2> <p>The primary objective of this project is to implement and evaluate a GNN model that can predict whether a social tie will form between two users in a social network. Secondary objectives include:</p> <ul> <li>Investigating the features that are most predictive of tie formation.</li> <li>Assessing the role of network topology in influencing prediction accuracy.</li> <li>Evaluating the feasibility of using tie predictions as a proxy for trust.</li> </ul> <h2 id="methods">Methods</h2> <p>We will employ a publicly available social network dataset, pre-process it to suit our needs, and construct a GNN model using a framework such as PyTorch Geometric. The model will be trained to predict links between nodes, with performance measured by accuracy, precision, recall, and F1 score.</p> <h2 id="data">Data</h2> <p>The dataset will be sourced from a reputable public repository (SNAP) that contains social network graphs with node and edge attributes. Suitable candidates include datasets from platforms such as Twitter or academic collaboration networks.</p> <h2 id="expected-outcomes">Expected Outcomes</h2> <p>The project aims to demonstrate the capability of GNNs in accurately predicting social ties. The expected outcome is a model with robust predictive performance that could potentially be deployed in a real-world social network setting to suggest new connections or detect communities.</p> <h2 id="timeline">Timeline</h2> <ul> <li><strong>Week 1</strong>: Literature review and dataset procurement.</li> <li><strong>Week 2</strong>: GNN architecture definition</li> <li><strong>Week 3</strong>: Data cleaning, preprocessing, and exploratory data analysis.</li> <li><strong>Week 4</strong>: Implementation of the GNN model, initial training, and hyperparameter tuning.</li> <li><strong>Week 5</strong>: Final model training, evaluation, and analysis of results. Preparation of the project report and presentation.</li> </ul> <h2 id="summary-and-literature">Summary and Literature</h2> <p>This project stands to contribute valuable insights into the application of Graph Neural Networks to social network analysis, specifically in the prediction of social ties which may correlate with trust. The findings could have implications for the design of social media platforms, recommendation systems, and the broader field of network science.</p> <p>This project on leveraging Graph Neural Networks (GNNs) for predicting social connections, serving as proxies for trust, is substantiated by insights from works in the field. The study ‘A Deep Graph Neural Network-Based Mechanism for Social Recommendations’ by Guo and Wang, alongside ‘Rec-GNN: Research on Social Recommendation based on Graph Neural Networks’ by Si et al., both underscore the efficacy of GNNs in social recommendation systems. These articles illustrate how GNNs can effectively decipher complex social interactions, an aspect crucial to this project’s focus on trust prediction within social networks. Furthermore, ‘A Survey of Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions’ by Gao et al. offers a comprehensive landscape of GNN applications in recommendation scenarios, highlighting both challenges and future directions. This survey provides a broad understanding of GNN methodologies and potential pitfalls, thereby enriching the approach towards modeling trust through social connections. Collectively, these sources not only offer theoretical backing but also practical insights into the application of GNNs in understanding and predicting the dynamics of social networks.</p> <hr/>]]></content><author><name></name></author><category term="project"/><category term="deep_learning"/><category term="graph_neural_networks"/><summary type="html"><![CDATA[Project Proposal: Predicting Social Ties Using Graph Neural Networks]]></summary></entry><entry><title type="html">(Proposal) Physics-informed Learning for Chaotic Dynamics Prediction</title><link href="https://deep-learning-mit.github.io/staging/blog/2023/Physics-informed-learning-chaotic-dynamics/" rel="alternate" type="text/html" title="(Proposal) Physics-informed Learning for Chaotic Dynamics Prediction"/><published>2023-11-10T00:00:00+00:00</published><updated>2023-11-10T00:00:00+00:00</updated><id>https://deep-learning-mit.github.io/staging/blog/2023/Physics-informed-learning-chaotic-dynamics</id><content type="html" xml:base="https://deep-learning-mit.github.io/staging/blog/2023/Physics-informed-learning-chaotic-dynamics/"><![CDATA[<h2 id="project-overview">Project Overview</h2> <p>In this project, we would like to explore how to incorporate physics-based prior knowledge in to machine learning models for dynamical systems. Traditionally, physics laws have been used to model system behaviors with a set of differential equations, e.g. using Newton’s second law to derive a pendulum equation of motion, or using Navior Stoke’s equations to describe air flow in space. However, such physics-based modeling methods become challenging to implement for complex systems as all physics-based models come with assumption that helps simplify the scenario to certain extent. In recent years, machine learning has shown great potentials for developing data-driven modeling methods <d-cite key="brunton2022data"></d-cite>.</p> <p>Although learning-based methods have shown their capability of generating accuracy prediction of dynamical systems, the learned representations are difficult to interpret, especially when general multi-layer perceptron (MLP) or recurrent neural network (RNN) are used to construct the estimator. Apart from an accurate prediction, interpretability is also desirable as it helps us understand the limitation of such models. Furthermore, if a model can be constructed in line with physical modeling principles, ideally it might reveal more structured information about the data we collected from a given system. One might even hope an interpretable machine learning model would give us new insights about how to construct efficient models and discover new physics properties about a dynamical system from data.</p> <p>To narrow the scope of the problem for feasibility of this course project, we will focus on the long-term prediction problem for a deterministic chaotic system, Lorenz 63, first proposed and studied in E. N. Lorenz’s seminal paper <d-cite key="lorenz1963deterministic"></d-cite>. This system can be described in closed-form as a set of ordinary differential equations (ODE) with three variables, which makes the learning problem less data hungry and easier to train neural networks for its prediction with limited computation power. Despite the chaotic nature of the system (meaning that a small perturbation to the system can lead to exponential divergence in time from its original trajectory), the state of Lorenz 63 stays on a “strange attractor”” (a bounded set in the state space as shown in the animation below). We refer to the fact that the trajectory stays on the attractor as the “long-term” stability of Lorenz 63. Such long-term stability is desirable for any predictor as it indicates learning about statistical behavior of the system. Methods that can guarantee such long-term stability based on machine learning have not appeared so far, but theoretical guarantees are highly desirable as they are part of the intrinsic system properties and indicate meaningfulness of our learnt representations. Furthermore, Lorenz 63 is a simplified version of complex atmosphere thermodynamics models which are crucial in climate studies or weather forecasting. Starting with Lorenz 63 is a meaningful gateway to studying physics-informed learning approaches for climate models.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-10-Physics-informed-learning-chaotic-dynamics/Lorenz63-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-10-Physics-informed-learning-chaotic-dynamics/Lorenz63-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-10-Physics-informed-learning-chaotic-dynamics/Lorenz63-1400.webp"/> <img src="/staging/assets/img/2023-11-10-Physics-informed-learning-chaotic-dynamics/Lorenz63.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> By simulating the closed-form ODE of Lorenz 63, it is observed that the trajectory always stay in the bounded region formed by these white-colored "orbits", the region is also known as the "strange attractor" of Lorenz 63. (The butterfly shape is beautiful!) </div> <p>Focused on the specific Lorenz 63 system, the objective of this project is to explore machine learning model structures that attempt to achieve two goals: (1) High prediction accuracy of the state trajectory (2) Provide theoretical guarantees for long-term stability, i.e., predicted trajectory stays on the “strange attractor”. In the literature, there has been approaches that use certain empirical methods to encourage long-term stability such as using noise regularization <d-cite key="wikner2022stabilizing"></d-cite>. However, such methods do not offer any theoretical guarantees and are generally difficult to interpret. We aim to investigate a specific model construction that incorporates “energy” information of the system, analogous to a recent approach in stability-guaranteed learning-based approach in control theory <d-cite key="min2023data"></d-cite>. On a high level, the proposed approach tries to learn both a predictor for Lorenz 63 and a “energy” function, and constructs a neural network for the predictor with specific activation functions such that it is constrained to a non-increasing energy condition (we will provide a more detailed description in the next section). The goal is to investigate whether this idea works on Lorenz 63 system, what type of structure we need to impose on the neural network to achieve both goals, and whether constraining the network structure leads to a trade-off between the theoretical guarantees and prediction accuracy.</p> <h2 id="problem-formulation">Problem Formulation</h2> <p>Consider a general continnuous-time nonlinear dynamics system (we will use continuous-time dynamical system formulation throughout the project):</p> \[\dot{s}(t) = f(s(t)), s(t) \in \mathbb{R}^n\] <p>The objective of a general prediction problem is to learn a neural network-based function approximator \(g: \mathbb{R}^n \to \mathbb{R}^n\) such that the ODE \(\dot{s}(t) = g(s(t))\) approximates the true system above well. Namely, suppose we simulate both ODEs from the same initial condition \(r(0) = s(0)\), we want the predicted trajectory \(r(t)\), which is generated by \(\dot{r}(t) = g(r(t))\) to approximate \(x(t)\) well, i.e., \(\sup_{t \geq 0} \|r(t) - s(t)\|\) to be small.</p> <p>Specifically, here we consider the Lorenz 63 system, which can be described as (here \(x, y, z\) are scalar variables)</p> \[\begin{align*} \dot{x} &amp;= \sigma (y-x)\\ \dot{y} &amp;= x(\rho - z) - y\\ \dot{z} &amp;= xy - \beta z \end{align*}\] <p>where \(\sigma, \rho, \beta\) are scalar parameters for the system. We choose \(\sigma=10, \beta=8/3, \rho=28\) as they generate chaotic behaviors and still observe long-term stability.</p> <h3 id="a-motivating-example">A motivating example</h3> <p>We first consider a set of motivating numerical experiments which build a simple 3-layer MLP as a predictor for discrete-time Lorenz 63 to assess how difficult it is to approximate the dynamics. (Apologies for potential confusions, we use discrete-time systems because it’s easier to set up, but we will use continuous-time systems in the project.) The discrete-time system is numerically integrated from the continuous-time version using 4th order Runge-Kutta method (RK4) sampled at a fixed time step \(\Delta t\), which is in the form of</p> \[s[k+1] = f_d(s[k]), s[k] = s(k\Delta t) \in \mathbb{R}^3\] <p>We generate a dataset by sampling \(N\) one-step pair \((s[k], s[k+1]), k = 0, 1, 2, ..., N-1\) from a single long trajectory using the discrete-time dynamics. A 3-layer MLP \(g(s[k]; \theta)\) (parameterized by weights \(\theta\)) is trained to minimize the MSE loss via SGD, i.e.,</p> \[\min_{\theta} \frac{1}{N} \sum_{k=0}^{N-1} \|s[k+1] - g(s[k]; \theta)\|_2^2\] <p>During testing, we choose a initial condition \(s[0]\), different than the one used to generate the training data, and generate a ground-truth trajectory of step \(N\) as the testing dataset \(\{s[n]\}_{n=0}^{N-1}\) and use the trained network by generating two separate trajectories as follows:</p> <ol> <li> <p>“MLP One-step”: we apply the network to the ground-truth \(s[n]\) at every step, i.e., the trajectory \(s_1[n]\) that we generate satisfies \(s_1[0] = s[0]\) and \(s_1[k+1] = g(s[k])\).</p> </li> <li> <p>“MLP Feedback”: we set the initial condition \(s_2[0] = s[0]\) and apply the network prediction iteratively, i.e., \(s_2[k+1] = g(g(... g(s[0])))\) where \(g\) is applied \(k\) times.</p> </li> </ol> <p>To reduce the length of this post, we only present two most relevant examples here. When we have a dataset of \(N=1000\) sampled one-step pairs, using GeLU activation, we are able to achieve very good prediction accuracy in both cases and both trajectories observe the “strange attractor” long-term stability as desired.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-10-Physics-informed-learning-chaotic-dynamics/Gelu_1000-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-10-Physics-informed-learning-chaotic-dynamics/Gelu_1000-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-10-Physics-informed-learning-chaotic-dynamics/Gelu_1000-1400.webp"/> <img src="/staging/assets/img/2023-11-10-Physics-informed-learning-chaotic-dynamics/Gelu_1000.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Both prediction accuracy and long-term stability achieved when $$N=1000$$. </div> <p>However, when we reduce the dataset to \(N=100\) sampled one-step pairs, using the same GeLU activation, the “MLP feedback” trajectory fails to make accurate prediction and long-term stability. Meanwhile, the “MLP one-step” trajectory still makes very good one-step prediction. This implies that the training problem is solved almost perfectly, however, due to the nature of chaotic dynamics, a little divergence from the true dynamics, when rolled out in \(N\) steps (as in the setting of “feedback”), it diverge from the true trajectory very quickly.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-10-Physics-informed-learning-chaotic-dynamics/Gelu_100-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-10-Physics-informed-learning-chaotic-dynamics/Gelu_100-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-10-Physics-informed-learning-chaotic-dynamics/Gelu_100-1400.webp"/> <img src="/staging/assets/img/2023-11-10-Physics-informed-learning-chaotic-dynamics/Gelu_100.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> When $$N=100$$, "MLP feedback" fails while the training problem is solved well. (the blue and green trajectories overlap with each other) </div> <p>Although there are more advanced ways of learning a time-series data like this, e.g., RNNs, this simplified exercise illustrates the difficulty of learning an underlying structure of dynamics (rolling out the trajectory iteratively) compared to fitting data to achieve near-zero MSE loss (one-step prediction), especially when data is limited.</p> <p>The setup of “feedback” is meaningful in a practical sense. For applications such as climate modeling, we typically want to learn what would happen in the future (in months/years), therefore, we cannot use a “one-step” prediction setup where we are restricted to predicting events in a very small future time window.</p> <h3 id="physics-informed-modeling">Physics-informed modeling</h3> <p>As mentioned in the previous section, we aim to explore physics-informed network structures that impose certain physical constraints, with a focus on developing a method similar to the one proposed in <d-cite key="min2023data"></d-cite>. Here in the proposal, we will give a quick outline of what this approach might look like (The actual approach will be developed fully in this project).</p> <p>If we look back on the Lorenz 63 equation (continuous-time), it is not difficult to see that on the right hand side, we have a second-order polynomial of the state. Therefore, if we consider the following energy function \(V\) and write out its time derivative \(\dot{V} = dV/dt = \partial V/\partial [x, y, z] [\dot(x), \dot(y), \dot(z)]^T\), we have</p> \[\begin{align*} V &amp;= \rho x^2 + \sigma y^2 + \sigma(z - 2\rho)^2\\ \dot{V} &amp;= -2\sigma( \rho x^2 + y^2 + \beta(z-\rho)^2 - \beta \rho^2) \end{align*}\] <p>Note that \(V\) is always a non-negative function, and outside an ellipsoid \(E = \{(x, y, z): \rho x^2 + y^2 + \beta (z - \rho)^2 \leq \beta \rho^2\}\), \(\dot{V}\) is always smaller than 0, i.e., \(\forall (x, y, z) \not\in E\), \(\dot{V}(x, y, z) &lt; 0\).</p> <p>This is actually one interpretation why the Lorenz 63 system always stay on a bounded “strange attractor”, because its trajectory always loses energy when it is outside the set \(E\). Conceptually, the trajectory will always return to a certain energy level after it exits \(E\).</p> <p>Suppose we can construct a neural network \(g\) for the continuous-time dynamics and another neural network \(h\) for the energy function \(V(x, y, z)\), i.e.,</p> \[(\hat{\dot{x}}, \hat{\dot{y}}, \hat{\dot{z}}) = g( x, y, z ; \theta_g), \quad \hat{V}(x, y, z) = h( x, y, z ; \theta_h)\] <p>In a very similar context, <d-cite key="min2023data"></d-cite> developes a specific neural network structure for \(h\) that can ensure</p> \[\dot{h} = (\partial h(x, y, z; \theta)/\partial (x, y, z)) \cdot g(x, y, z; \theta_g) &lt; -\alpha h(x, y, z; \theta)\] <p>where \(\alpha &gt; 0\) is a positive scalar (for interested readers, this condition defines a Lyapunov function in control theory).</p> <p>In this project, we aim to develop a similar structure to ensure a slightly different (local) condition:</p> <p>\(\forall (x, y, z) \not\in E\), \(\dot{\hat{V}}(x, y, z) &lt; 0\).</p> <p>which constaints the learned model to satisfy a physical property of the system by construction. With such constraints implemented by construction, we can use the MSE loss similar to the motivating example to train both \(g\) and \(h\) simultaneously. Hopefully this would lead us to learning a network that achieves high prediction accuracy while obeying physical constraints.</p> <h2 id="project-scope">Project Scope</h2> <p>In the previous section, we gave an outline about why we want to investigate physics-based modeling for Lorenz 63 and what specific physical system information we would like to incorporate. Although we plan to spend a decent amount of time to implement and test the specific method mentioned previously, we would like to reiterate the project objective and its possible relevance to this course in this section.</p> <p>The project’s general objective is to investigate how to learn meaningful physics-informed representations and build constrained machine learning models that ensure certain physical properties. Picking the specific problem and approach helps us focus on a more concrete problem, but it does not restrict the project to implementation of this specific method.</p> <p>More importantly, since the proposed method uses specific activation functions in <d-cite key="min2023data"></d-cite> to impose physical constraints, it restricts our model to a smaller class defined by such constraints. There could be several interesting questions downstream to be investigated:</p> <ul> <li>Would the constrained class of models be able to achieve high prediction accuracy?</li> <li>Is there a trade-off between physics constraint satisfaction (model class) and prediction accuracy (minimizing MSE loss)?</li> <li>Does the physics-informed model provide acceptable prediction accuracy in the limited data regime?</li> <li>After training, what does the \(h\) network learn? Does it resemble an energy function?</li> </ul> <p>Furthermore, we would also perform a short literature review to survey other physics-informed learning methods for dynamical systems. If we find a highly relevant approach that would work for problem, under the time constraint of the project, we will try to implement such approaches and compare our approach with them as well.</p> <h2 id="research-project-disclaimers">Research Project Disclaimers</h2> <p>I would like to inform the teaching staff that this project is planned to be part of my ongoing research. During the semester, I don’t have much time to work on this idea as I am trying to meet a conference deadline for another ongoing project. Since the project explores learning efficient physical representations for dynamical system, I am hoping that I can use the course project opportunity to work on this idea. There has not been much prior work done except the thought process presented in this proposal. If the specific approach proposed turns out to be successful, I would like to extend it into my next research project and hopefully part of my Ph.D. thesis.</p> <p>Please let me know if this would be acceptable under the course guideline. I’d be happy to make other modifications to follow the course project guideline on using ideas relevant to ongoing/future research.</p>]]></content><author><name>Sunbochen Tang</name></author><summary type="html"><![CDATA[Project proposal submission by Sunbochen Tang.]]></summary></entry><entry><title type="html">GINTransformer vs. Bias</title><link href="https://deep-learning-mit.github.io/staging/blog/2023/distill-example/" rel="alternate" type="text/html" title="GINTransformer vs. Bias"/><published>2023-11-10T00:00:00+00:00</published><updated>2023-11-10T00:00:00+00:00</updated><id>https://deep-learning-mit.github.io/staging/blog/2023/distill-example</id><content type="html" xml:base="https://deep-learning-mit.github.io/staging/blog/2023/distill-example/"><![CDATA[<h2 id="proposal">Proposal</h2> <p>The first piece of information that a person recieves about a given topic determines their belief as a whole on said topic. This is shown in expirements where participants beliefs on several topics were challenged with empirical evidence against their beliefs. Studies consistently show that one a person has their mind made up, it is significantly more difficult to change their mind everytime you challenge them on it. Every interaction solidifies their belief. This is epseically important in the context of the social media era we are living in. A lot of the time, people’s first impressions over a given event gets primed by what they see about it on theif feeds. This is coming to determine more and more discourse, and especially so when global events occur and those under duress can now more broadly share their stories and struggles. While good, we also have to contend with oppositional, orpessive forces using thise to boon their politic. Being able to determine the source of a given topic, or being able to filter through accounts with troublesome history, would bridge the misinformation gap that has always been a problem long before the social networks of the day.</p> <p>To measure this information flow, I propose using a GIN-Based Transformer implimentation to tackle misinformation detection and tracking. The dataset will be constructed from a few years of social media activity in clusters between active users. While the age dunamics across social media apps vary greatly, I predict that a similar trend in misinformation will appear once we abstract away all the noise. I am choosing to implement this using a GIN because I want to take advantage of the network architectures isomorphism property to create non-sparse dense connections for the transformer network to take advantage of to the fullest with multi-headed attention. Each node in the network will comprise tweets and character profiles attached to them, giving context for the post content. I want to exploit this structure to determine the underlying trends that determine communication online.</p> <p>Detecting misinformation is hard. The problem on in the internet age is that detecting misinformation is akin to detecting whether a given claim is true or not, esentially lie detection. This, understandably is really difficult to do even with fact checkers because sometimes, there simply is no one that knows what the whole truth is. Instead of trying to tackle misinformation directly, this proposed approach works to analyze underlying trends in the profiles of people that typically engage in spreading misinformation, and the typical structure that said misinformation takes–a metric i define as information density. Information density will serve to measure the level to which there is a correspondence between the models measure of the veracity of a given claim and the models measure of the profile said text came from.</p> <p>I am hoping to find a robust way to compute the information density of a given account, text pair and use that to determine how trustworthy a given claim is based on previous percieved patterns. In additon to the architecture above, I will be using conditional prompting to augment my data and will finetune my transformer network for the tweets using Distilbert. I want the model to be as light weight and portable as possible, as such I want the predictive ability of my network to not be costly.</p>]]></content><author><name>Yeabsira Moges</name></author><summary type="html"><![CDATA[Your blog post's abstract. This is an example of a distill-style blog post and the main elements it supports.]]></summary></entry></feed>